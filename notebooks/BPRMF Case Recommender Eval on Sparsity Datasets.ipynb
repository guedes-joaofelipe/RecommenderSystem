{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MostPopular Template for Sparsity Evaluation\n",
    "\n",
    "## Imports "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys, joblib, json, time, re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt \n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "%matplotlib inline \n",
    "from caserec.recommenders.item_recommendation.bprmf import BprMF\n",
    "from random import choice\n",
    "from string import ascii_uppercase\n",
    "from IPython.display import clear_output\n",
    "lib_path = './../Sources'\n",
    "if (lib_path not in sys.path):\n",
    "    sys.path.append(lib_path) #src directory\n",
    "from lpsrec.messaging.print_functions import ProgressBar\n",
    "from lpsrec.messaging.telegrambot import Bot\n",
    "from lpsrec.utils import partition_dataframe, write_log\n",
    "import lpsrec.database as db"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters\n",
    "These parameters are set by a [Papermill](https://github.com/nteract/papermill) runner script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "dataset_tag = 'ML100k'\n",
    "model_tag = 'BprMF'\n",
    "rank_length = 30\n",
    "random_state = 31415\n",
    "evaluation_metrics = ['PREC', 'RECALL', 'NDCG', 'MRR', 'MAP']\n",
    "bot_alive = False\n",
    "partition = 1\n",
    "nodes = 10\n",
    "n_folds = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis_tag = '_'.join([str(x) for x in [rank_length, nodes, partition]])\n",
    "n_folds = None if n_folds == 1 else n_folds\n",
    "style_dict = json.load(open('./style_dict.json', 'r'))\n",
    "dataset_output_folder = os.path.join('.', 'Outputs', dataset_tag)\n",
    "variables_output_folder = os.path.join(dataset_output_folder, model_tag, 'Variables', analysis_tag)\n",
    "figures_output_folder = os.path.join(dataset_output_folder, model_tag, 'Figures', analysis_tag)\n",
    "progbar = ProgressBar(bar_length=20, bar_fill='#', elapsed_time=True)\n",
    "if not os.path.exists(variables_output_folder):    \n",
    "    os.makedirs(variables_output_folder)    \n",
    "if not os.path.exists(figures_output_folder):        \n",
    "    os.makedirs(figures_output_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rc('font', **style_dict['font'])\n",
    "# plt.rc('axes.titlesize', fontsize=20)\n",
    "plt.rc('xtick', labelsize=style_dict['tick']['fontsize']) \n",
    "plt.rc('ytick', labelsize=style_dict['tick']['fontsize']) \n",
    "plt.rcParams.update({'figure.max_open_warning': 5})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bot = Bot(user_credentials='./JFGS.json')\n",
    "bot.send_message(text=\"{}\\nHello, John. Initiating sparsity analysis for the {} dataset on the {} model [{}/{}]\".format('-'*20, dataset_tag, model_tag, partition, nodes)) if bot_alive else ''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connecting to Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "username = 'postgres'\n",
    "password = 'admin'\n",
    "dbname = 'RecSys'\n",
    "hostname = 'localhost:5432'\n",
    "conn = db.get_database_connection(username, password, hostname, dbname)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "df_ratings = db.get_dataset_from_sparsity(data_path=None, conn=conn, dataset_tag=dataset_tag) \n",
    "df_ratings[['feedback_value']] = df_ratings[['feedback_value']].apply(pd.to_numeric)\n",
    "df_ratings.drop(['user', 'item', 'timestamp'], axis=1, inplace=True)\n",
    "df_ratings.columns = ['feedback_value', 'user', 'item']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100000, 3)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feedback_value</th>\n",
       "      <th>user</th>\n",
       "      <th>item</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   feedback_value  user  item\n",
       "0               5     1     1\n",
       "1               3     1     2\n",
       "2               4     1     3\n",
       "3               3     1     4\n",
       "4               3     1     5"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print (df_ratings.shape)\n",
    "df_ratings.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: 70000 \n",
      "Test size: 30000\n"
     ]
    }
   ],
   "source": [
    "df_train, df_test = train_test_split(df_ratings, test_size=0.3, random_state=random_state)\n",
    "print (\"Train size: {} \\nTest size: {}\".format(df_train.shape[0], df_test.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BprMF(train_file=df_train, test_file=df_test, rank_length=rank_length, factors=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "model.compute(verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr_k = np.arange(1, rank_length+1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if partition == 1: \n",
    "    model.evaluate(metrics=evaluation_metrics, n_ranks = arr_k)\n",
    "    joblib.dump(model.evaluation_results, os.path.join(variables_output_folder, 'evaluation_results.joblib'))\n",
    "\n",
    "    evaluation_results = joblib.load(os.path.join(variables_output_folder, 'evaluation_results.joblib'))\n",
    "    fig, ax = plt.subplots(nrows=1, ncols=1, figsize=(10,10))\n",
    "\n",
    "    for evaluation_name in [m + '@' for m in evaluation_metrics]:\n",
    "        ax.plot(arr_k, [evaluation_results[evaluation_name + str(x)] for x in arr_k], label=evaluation_name + 'k', color=style_dict['evaluations'][evaluation_name + 'k'])\n",
    "\n",
    "    ax.set_xticks(np.arange(0, np.max(arr_k)+1, 2), minor=False)\n",
    "    ax.set_xlabel('Rank k', fontsize = style_dict['label']['fontsize'])\n",
    "    ax.tick_params(axis='both', which='major', labelsize=style_dict['tick']['fontsize'])\n",
    "    ax.legend()\n",
    "    ax.grid(True)\n",
    "    filepath = os.path.join(figures_output_folder, 'item_rec_metrics.png')\n",
    "    plt.savefig(filepath, bbox_inches = 'tight')\n",
    "    bot.send_message(filePath=filepath) if bot_alive else ''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating on Sparsity Cenarios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset MovieLens 100k (id_dataset = 1) with 100000 registers\n",
      "Number of sparsity cenarios:  1296\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id_sparsity_cenario</th>\n",
       "      <th>uss</th>\n",
       "      <th>iss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10759</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10760</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10761</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10762</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10763</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.38</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id_sparsity_cenario  uss   iss\n",
       "0                10759  0.3  0.30\n",
       "1                10760  0.3  0.32\n",
       "2                10761  0.3  0.34\n",
       "3                10762  0.3  0.36\n",
       "4                10763  0.3  0.38"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_info = pd.read_sql(con=conn , sql=\"select * from datasets.dataset where version='{}'\".format(dataset_tag))\n",
    "df_sparsity_cenario = pd.read_sql(con=conn , sql=\"select * from sparsity.cenario\")\n",
    "sql_str = \"select id_user, id_item from sparsity.get_dataset_from_sparsity('{}', 1.0, 1.0)\".format(dataset_tag)\n",
    "df_whole = pd.read_sql(con=conn, sql=sql_str)\n",
    "print (\"Dataset {} (id_dataset = {}) with {} registers\".format(dataset_info['name'][0], dataset_info['id_dataset'][0], df_whole.shape[0]))\n",
    "print (\"Number of sparsity cenarios: \", df_sparsity_cenario.shape[0])\n",
    "df_sparsity_cenario.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id_user_dataset</th>\n",
       "      <th>uss</th>\n",
       "      <th>n_feedback</th>\n",
       "      <th>max_feedback</th>\n",
       "      <th>id_user</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>363</th>\n",
       "      <td>364</td>\n",
       "      <td>0.972863</td>\n",
       "      <td>20</td>\n",
       "      <td>737</td>\n",
       "      <td>364</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>823</th>\n",
       "      <td>824</td>\n",
       "      <td>0.972863</td>\n",
       "      <td>20</td>\n",
       "      <td>737</td>\n",
       "      <td>824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>811</th>\n",
       "      <td>812</td>\n",
       "      <td>0.972863</td>\n",
       "      <td>20</td>\n",
       "      <td>737</td>\n",
       "      <td>812</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>241</th>\n",
       "      <td>242</td>\n",
       "      <td>0.972863</td>\n",
       "      <td>20</td>\n",
       "      <td>737</td>\n",
       "      <td>242</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>808</th>\n",
       "      <td>809</td>\n",
       "      <td>0.972863</td>\n",
       "      <td>20</td>\n",
       "      <td>737</td>\n",
       "      <td>809</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    id_user_dataset       uss  n_feedback  max_feedback  id_user\n",
       "363             364  0.972863          20           737      364\n",
       "823             824  0.972863          20           737      824\n",
       "811             812  0.972863          20           737      812\n",
       "241             242  0.972863          20           737      242\n",
       "808             809  0.972863          20           737      809"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sql_str = \"\"\"select * from sparsity.get_dataset_uss('{}')\"\"\".format(dataset_tag)\n",
    "df_users_uss = pd.read_sql(con=conn, sql=sql_str)\n",
    "df_users_uss.sort_values(['uss'], ascending=False).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id_item_dataset</th>\n",
       "      <th>iss</th>\n",
       "      <th>n_feedback</th>\n",
       "      <th>max_feedback</th>\n",
       "      <th>id_item</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1681</th>\n",
       "      <td>1682</td>\n",
       "      <td>0.998285</td>\n",
       "      <td>1</td>\n",
       "      <td>583</td>\n",
       "      <td>1682</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>676</th>\n",
       "      <td>677</td>\n",
       "      <td>0.998285</td>\n",
       "      <td>1</td>\n",
       "      <td>583</td>\n",
       "      <td>677</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>710</th>\n",
       "      <td>711</td>\n",
       "      <td>0.998285</td>\n",
       "      <td>1</td>\n",
       "      <td>583</td>\n",
       "      <td>711</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1586</th>\n",
       "      <td>1587</td>\n",
       "      <td>0.998285</td>\n",
       "      <td>1</td>\n",
       "      <td>583</td>\n",
       "      <td>1587</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1585</th>\n",
       "      <td>1586</td>\n",
       "      <td>0.998285</td>\n",
       "      <td>1</td>\n",
       "      <td>583</td>\n",
       "      <td>1586</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     id_item_dataset       iss  n_feedback  max_feedback  id_item\n",
       "1681            1682  0.998285           1           583     1682\n",
       "676              677  0.998285           1           583      677\n",
       "710              711  0.998285           1           583      711\n",
       "1586            1587  0.998285           1           583     1587\n",
       "1585            1586  0.998285           1           583     1586"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sql_str = \"\"\"select * from sparsity.get_dataset_iss('{}')\"\"\".format(dataset_tag)\n",
    "df_items_iss = pd.read_sql(con=conn, sql=sql_str)\n",
    "df_items_iss.sort_values(['iss'], ascending=False).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "uss_limits = df_sparsity_cenario['uss'].unique()\n",
    "iss_limits = df_sparsity_cenario['iss'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feedback_value</th>\n",
       "      <th>user</th>\n",
       "      <th>item</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   feedback_value  user  item\n",
       "0               5     1     1\n",
       "1               3     1     2\n",
       "2               4     1     3"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_ratings.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input sparsity cenario dataframe shape:  (130, 5)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>uss_limit</th>\n",
       "      <th>iss_limit</th>\n",
       "      <th>os</th>\n",
       "      <th>num_users</th>\n",
       "      <th>num_items</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.36</td>\n",
       "      <td>0.34</td>\n",
       "      <td>0.074074</td>\n",
       "      <td>9.0</td>\n",
       "      <td>15.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.38</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>9.0</td>\n",
       "      <td>12.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.40</td>\n",
       "      <td>0.44</td>\n",
       "      <td>0.091667</td>\n",
       "      <td>10.0</td>\n",
       "      <td>24.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.38</td>\n",
       "      <td>0.48</td>\n",
       "      <td>0.103704</td>\n",
       "      <td>9.0</td>\n",
       "      <td>30.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.34</td>\n",
       "      <td>0.48</td>\n",
       "      <td>0.109524</td>\n",
       "      <td>7.0</td>\n",
       "      <td>30.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   uss_limit  iss_limit        os  num_users  num_items\n",
       "0       0.36       0.34  0.074074        9.0       15.0\n",
       "1       0.38       0.30  0.083333        9.0       12.0\n",
       "2       0.40       0.44  0.091667       10.0       24.0\n",
       "3       0.38       0.48  0.103704        9.0       30.0\n",
       "4       0.34       0.48  0.109524        7.0       30.0"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_overall_sparsity = pd.read_csv(os.path.join('.', 'Outputs', dataset_tag, 'OS', 'Variables', 'df_overall_sparsity.tsv'), sep = '\\t', header = 0)\n",
    "if partition is not None:\n",
    "    df_overall_sparsity = partition_dataframe(df=df_overall_sparsity, nodes=nodes, sort_by='os', ascending=True)[partition-1].reset_index(drop = True)\n",
    "print (\"Input sparsity cenario dataframe shape: \", df_overall_sparsity.shape)\n",
    "df_overall_sparsity.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_evaluation_results(model, df_train, df_test, rank_length):\n",
    "    model = BprMF(train_file=df_train, test_file=df_test, rank_length=rank_length, factors=30)\n",
    "    model.compute(verbose=False)\n",
    "    model.evaluate(metrics=evaluation_metrics, n_ranks = arr_k, verbose=False)\n",
    "    evaluation_results = pd.DataFrame.from_dict(data=model.evaluation_results, orient='index').T\n",
    "    evaluation_results['iss'] = [cenario['iss_limit']]\n",
    "    evaluation_results['uss'] = [cenario['uss_limit']]\n",
    "    return evaluation_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "tags": [
     "main_analysis"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4:10:43][####################] 99.23% Processing folder 1/5...\n",
      "\n",
      "Eval:: PREC@1: 0.461294 PREC@3: 0.423471 PREC@5: 0.400212 PREC@10: 0.36087 RECALL@1: 0.023037 RECALL@3: 0.064713 RECALL@5: 0.099873 RECALL@10: 0.173229 MAP@1: 0.461294 MAP@3: 0.577413 MAP@5: 0.575281 MAP@10: 0.534337 NDCG@1: 0.461294 NDCG@3: 0.680165 NDCG@5: 0.685382 NDCG@10: 0.661763 MRR@1: 0.461294 MRR@3: 0.586073 MRR@5: 0.610675 MRR@10: 0.621397 \n",
      "Processing folder 2/5...\n",
      "\n",
      "Eval:: PREC@1: 0.458112 PREC@3: 0.415695 PREC@5: 0.393001 PREC@10: 0.350053 RECALL@1: 0.02344 RECALL@3: 0.061675 RECALL@5: 0.095809 RECALL@10: 0.162903 MAP@1: 0.458112 MAP@3: 0.572817 MAP@5: 0.571699 MAP@10: 0.537382 NDCG@1: 0.458112 NDCG@3: 0.667111 NDCG@5: 0.672729 NDCG@10: 0.659265 MRR@1: 0.458112 MRR@3: 0.578826 MRR@5: 0.603376 MRR@10: 0.616272 \n",
      "Processing folder 3/5...\n",
      "\n",
      "Eval:: PREC@1: 0.466596 PREC@3: 0.434783 PREC@5: 0.407211 PREC@10: 0.361506 RECALL@1: 0.024472 RECALL@3: 0.0647 RECALL@5: 0.100044 RECALL@10: 0.168276 MAP@1: 0.466596 MAP@3: 0.580682 MAP@5: 0.58128 MAP@10: 0.548314 NDCG@1: 0.466596 NDCG@3: 0.678044 NDCG@5: 0.682552 NDCG@10: 0.671425 MRR@1: 0.466596 MRR@3: 0.588901 MRR@5: 0.612071 MRR@10: 0.624616 \n",
      "Processing folder 4/5...\n",
      "\n",
      "Eval:: PREC@1: 0.463415 PREC@3: 0.419583 PREC@5: 0.400212 PREC@10: 0.35684 RECALL@1: 0.022265 RECALL@3: 0.059619 RECALL@5: 0.094127 RECALL@10: 0.167265 MAP@1: 0.463415 MAP@3: 0.568399 MAP@5: 0.565892 MAP@10: 0.537071 NDCG@1: 0.463415 NDCG@3: 0.659461 NDCG@5: 0.661676 NDCG@10: 0.658941 MRR@1: 0.463415 MRR@3: 0.576352 MRR@5: 0.59894 MRR@10: 0.614771 \n",
      "Processing folder 5/5...\n",
      "\n",
      "Eval:: PREC@1: 0.450689 PREC@3: 0.413927 PREC@5: 0.394698 PREC@10: 0.355567 RECALL@1: 0.022191 RECALL@3: 0.059275 RECALL@5: 0.094413 RECALL@10: 0.164468 MAP@1: 0.450689 MAP@3: 0.564687 MAP@5: 0.562337 MAP@10: 0.529205 NDCG@1: 0.450689 NDCG@3: 0.662856 NDCG@5: 0.664695 NDCG@10: 0.652016 MRR@1: 0.450689 MRR@3: 0.572817 MRR@5: 0.595829 MRR@10: 0.608489 \n",
      "Finished creating sparsity datasets [partition = 1] for\tML100k in\t4:10:43\n",
      "Wall time: 4h 26min 8s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "write_log(filepath=os.path.join(variables_output_folder, 'log.txt'), \n",
    "          mode=\"w+\", \n",
    "          text='[{}]\\tUSS \\tISS \\tIndex\\tPerc\\tFolder\\n'.format(time.strftime(\"%Y-%m-%d %H:%M:%S\", time.gmtime())))    \n",
    "\n",
    "arr_k = np.arange(1, rank_length+1, 1)\n",
    "arr_df_eval_metadata = np.repeat(None, 1) if n_folds == None else np.repeat(None, n_folds)\n",
    "first_value = [True] if n_folds == None else np.repeat(True, n_folds)\n",
    "kf = None if n_folds == None else KFold(n_splits=n_folds, random_state=random_state) \n",
    "# for uss_index, uss_limit in enumerate(uss_limits):\n",
    "#     for iss_limit in iss_limits:   \n",
    "for index, cenario in df_overall_sparsity.iterrows():\n",
    "    clear_output()\n",
    "    progbar.update_progress(index/float(df_overall_sparsity.shape[0]))   \n",
    "\n",
    "    start_time = time.time()\n",
    "    df_ratings = db.get_dataset_from_sparsity(conn=conn, dataset_tag=dataset_tag,                                               \n",
    "                                              iss=cenario['iss_limit'], \n",
    "                                              uss=cenario['uss_limit'])\n",
    "\n",
    "    df_ratings.drop(['user', 'item', 'timestamp'], axis=1, inplace=True)    \n",
    "    df_ratings[['feedback_value']] = df_ratings[['feedback_value']].apply(pd.to_numeric)\n",
    "    df_ratings.columns = ['feedback_value', 'user', 'item']    \n",
    "    \n",
    "    if n_folds == None:\n",
    "        df_train, df_test = train_test_split(df_ratings, test_size=0.3, random_state=random_state)\n",
    "        evaluation_results = get_evaluation_results(model, df_train, df_test, rank_length)            \n",
    "        elapsed_time = time.time() - start_time\n",
    "        evaluation_results['elapsed_time'] = [elapsed_time]\n",
    "        \n",
    "        if (first_value[0]):            \n",
    "            first_value[0] = False\n",
    "            arr_df_eval_metadata[0] = evaluation_results.copy()    \n",
    "        else:\n",
    "            arr_df_eval_metadata[0] = arr_df_eval_metadata[0].append(evaluation_results)    \n",
    "        write_log(filepath=os.path.join(variables_output_folder, 'log.txt'), \n",
    "                  text='[{}]\\t{:.02f}\\t{:.02f}\\t{}/{}\\t{:.02f}%\\t{}/{}\\n'.format(time.strftime(\"%Y-%m-%d %H:%M:%S\", time.gmtime()), \n",
    "                                                                                 cenario['uss_limit'], \n",
    "                                                                                 cenario['iss_limit'], \n",
    "                                                                                 index+1, \n",
    "                                                                                 df_overall_sparsity.shape[0], \n",
    "                                                                                 100*(index/float(df_overall_sparsity.shape[0])), \n",
    "                                                                                 1, \n",
    "                                                                                 1)\n",
    "                 )\n",
    "    else:\n",
    "        \n",
    "        index_folder = 1\n",
    "        # Tentar fazer um shuffle e voltar com essa estrategia\n",
    "#         for train_index, test_index in kf.split(df_ratings):              \n",
    "#             df_train, df_test = df_ratings.iloc[train_index], df_ratings.iloc[test_index]            \n",
    "\n",
    "        for index_folder in np.arange(1, n_folds+1, 1):\n",
    "            df_train, df_test = train_test_split(df_ratings, test_size=0.3, random_state=random_state+10*index_folder-1)\n",
    "            \n",
    "            print (\"Processing folder {}/{}...\\n\".format(index_folder, n_folds))\n",
    "            evaluation_results = get_evaluation_results(model, df_train, df_test, rank_length)            \n",
    "            elapsed_time = time.time() - start_time\n",
    "            evaluation_results['elapsed_time'] = [elapsed_time]\n",
    "\n",
    "            if (first_value[index_folder-1]):            \n",
    "                first_value[index_folder-1] = False\n",
    "    #             df_eval_metadata = evaluation_results.copy()\n",
    "                arr_df_eval_metadata[index_folder-1] = evaluation_results.copy()\n",
    "            else:\n",
    "                arr_df_eval_metadata[index_folder-1] = arr_df_eval_metadata[index_folder-1].append(evaluation_results)    \n",
    "            write_log(filepath=os.path.join(variables_output_folder, 'log.txt'), \n",
    "                      text='[{}]\\t{:.02f}\\t{:.02f}\\t{}/{}\\t{:.02f}%\\t{}/{}\\n'.format(time.strftime(\"%Y-%m-%d %H:%M:%S\", time.gmtime()), \n",
    "                                                                                     cenario['uss_limit'], \n",
    "                                                                                     cenario['iss_limit'], \n",
    "                                                                                     index+1, \n",
    "                                                                                     df_overall_sparsity.shape[0], \n",
    "                                                                                     100*(index/float(df_overall_sparsity.shape[0])), \n",
    "                                                                                     index_folder, \n",
    "                                                                                     n_folds)\n",
    "                     )\n",
    "            index_folder += 1\n",
    "\n",
    "for index, df_eval_metadata in enumerate(arr_df_eval_metadata):\n",
    "    folder_name = '1_1' if n_folds == None else str(n_folds) + '_' + str(index+1)\n",
    "    df_eval_metadata.reset_index(drop = True, inplace=True)\n",
    "    df_eval_metadata.to_csv(os.path.join(variables_output_folder, 'df_eval_metadata_fold{}.tsv'.format(folder_name)), sep = '\\t', header = True, index = False)\n",
    "text = \"Finished creating sparsity datasets [partition = {}] for\\t{} in\\t{}\".format(partition, dataset_tag, progbar.get_elapsed_time())\n",
    "bot.send_message(text=text) if bot_alive else ''       \n",
    "write_log(filepath=os.path.join(variables_output_folder, 'log.txt'), text=text)\n",
    "print (text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (\"Variables saved @ \", variables_output_folder)\n",
    "bot.send_message(text=\"End of analysis for the {} dataset\\n{}\".format(dataset_tag, '-'*20)) if bot_alive else ''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_________________"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_eval_metadata = pd.read_csv(os.path.join(variables_output_folder, 'df_eval_metadata.tsv'), sep = '\\t', header = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_eval_metadata.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# uss_limits = np.sort(np.array(df_eval_metadata['uss'].unique()))\n",
    "# iss_limits = np.sort(np.array(df_eval_metadata['iss'].unique()))\n",
    "# rank_lengths = np.arange(1, rank_length+1, 1) # Setting extra rank analysis\n",
    "\n",
    "# for rank in rank_lengths:\n",
    "#     arr_rank_metrics = [m + '@' + str(rank) for m in evaluation_metrics]\n",
    "#     for column in arr_rank_metrics:\n",
    "#         arr_metric = np.zeros([len(uss_limits), len(iss_limits)])\n",
    "#         for uss_index, uss_limit in enumerate(uss_limits):\n",
    "#             for iss_index, iss_limit in enumerate(iss_limits):            \n",
    "#                 arr_metric[uss_index, iss_index] = df_eval_metadata[(df_eval_metadata['uss'] == uss_limit) & (df_eval_metadata['iss'] == iss_limit)][column].reset_index(drop = True)[0]\n",
    "\n",
    "#         joblib.dump(arr_metric, os.path.join(variables_output_folder, 'arr_' + column.lower() + '_' + model_tag + '.joblib'))        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting Results from Sparsity Cenarios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cmapping = \"jet\"\n",
    "# tick_step = 5\n",
    "# figs = dict()\n",
    "# for metric in evaluation_metrics:\n",
    "#     figs[metric+'@'] = list()\n",
    "\n",
    "# for rank in rank_lengths:\n",
    "#     arr_rank_metrics = [m + '@' + str(rank) for m in evaluation_metrics]\n",
    "#     for column in arr_rank_metrics:\n",
    "#         arr_prec = joblib.load(os.path.join(variables_output_folder, 'arr_' + column.lower() + '_' + model_tag + '.joblib'))        \n",
    "\n",
    "#         fig, ax = plt.subplots(figsize=(style_dict['figure']['width'],style_dict['figure']['height']))    \n",
    "#         cax = plt.imshow(arr_prec, cmap=cmapping)\n",
    "#         plt.gca().invert_yaxis()\n",
    "#         cbar = plt.colorbar(cax, ticks = [x/100.0 for x in np.arange(0,1000,10)], shrink = 0.83)\n",
    "\n",
    "#         ax.set_xticklabels(uss_limits[0:len(uss_limits):tick_step])\n",
    "#         ax.set_yticklabels(iss_limits[0:len(uss_limits):tick_step])\n",
    "#         ax.set_xticks(np.arange(0, len(uss_limits), tick_step))    \n",
    "#         ax.set_yticks(np.arange(0, len(iss_limits), tick_step))    \n",
    "#         ax.set_xlabel('Last User Specific Sparsity', fontsize = style_dict['label']['fontsize'])\n",
    "#         ax.set_ylabel('Last Item Specific Sparsity', fontsize = style_dict['label']['fontsize'])    \n",
    "#         ax.tick_params(axis='both', which='major', labelsize=style_dict['tick']['fontsize'])\n",
    "#         cbar.set_label(column.title(), labelpad=-50,  y=1.08, rotation=0, fontsize = style_dict['label']['fontsize'])\n",
    "#         cbar.ax.tick_params(labelsize = style_dict['tick']['fontsize'])\n",
    "#         plt.clim(0, 1)\n",
    "#         plt.xticks(rotation = 'vertical')\n",
    "\n",
    "#         filename = '2d-' + column + '.png'       \n",
    "#         fig.savefig(os.path.join(figures_output_folder, filename), bbox_inches = 'tight')\n",
    "        \n",
    "#         fig.set_animated(True)\n",
    "#         figs[re.split(\"\\d\", column)[0]].append(fig)        \n",
    "#         if rank == rank_length: # Send only the target-analysis\n",
    "#             bot.send_message(filePath=os.path.join(figures_output_folder, filename)) if bot_alive else ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# for metric in evaluation_metrics:    \n",
    "#     filepaths = [os.path.join(figures_output_folder, '2d-' + column + '.png' ) for column in [metric + '@' + str(rank) for rank in rank_lengths]]\n",
    "#     output_filepath = os.path.join(figures_output_folder, '2d-' + metric + '@k' + '.gif' )\n",
    "#     create_gif(filepaths, output_filepath, duration=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from IPython import display\n",
    "# # display.HTML('<img src=\"{}\">'.format(output_filepath))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show_metric = lambda metric, k: figs[metric + '@'][int(k)-1]\n",
    "# interact(show_metric, k=widgets.IntSlider(min=1, max=rank_length, step=1, value=10), figs=figs, metric=evaluation_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__________"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
