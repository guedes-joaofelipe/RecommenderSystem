{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MostPopular Template for Sparsity Evaluation\n",
    "\n",
    "## Imports "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys, joblib, json, time, re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt \n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "%matplotlib inline \n",
    "from caserec.recommenders.item_recommendation.most_popular import MostPopular\n",
    "from random import choice\n",
    "from string import ascii_uppercase\n",
    "from IPython.display import clear_output\n",
    "lib_path = './../Sources'\n",
    "if (lib_path not in sys.path):\n",
    "    sys.path.append(lib_path) #src directory\n",
    "from lpsrec.messaging.print_functions import ProgressBar\n",
    "from lpsrec.messaging.telegrambot import Bot\n",
    "from lpsrec.utils import partition_dataframe, write_log\n",
    "import lpsrec.database as db"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters\n",
    "These parameters are set by a [Papermill](https://github.com/nteract/papermill) runner script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "dataset_tag = 'BOOKX'\n",
    "model_tag = 'MostPopular'\n",
    "rank_length = 30\n",
    "random_state = 31415\n",
    "evaluation_metrics = ['PREC', 'RECALL', 'NDCG', 'MRR', 'MAP']\n",
    "bot_alive = False\n",
    "partition = 1\n",
    "nodes = 10\n",
    "n_folds = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis_tag = '_'.join([str(x) for x in [rank_length, nodes, partition]])\n",
    "n_folds = None if n_folds == 1 else n_folds\n",
    "style_dict = json.load(open('./style_dict.json', 'r'))\n",
    "dataset_output_folder = os.path.join('.', 'Outputs', dataset_tag)\n",
    "variables_output_folder = os.path.join(dataset_output_folder, model_tag, 'Variables', analysis_tag)\n",
    "figures_output_folder = os.path.join(dataset_output_folder, model_tag, 'Figures', analysis_tag)\n",
    "progbar = ProgressBar(bar_length=20, bar_fill='#', elapsed_time=True)\n",
    "if not os.path.exists(variables_output_folder):    \n",
    "    os.makedirs(variables_output_folder)    \n",
    "if not os.path.exists(figures_output_folder):        \n",
    "    os.makedirs(figures_output_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rc('font', **style_dict['font'])\n",
    "# plt.rc('axes.titlesize', fontsize=20)\n",
    "plt.rc('xtick', labelsize=style_dict['tick']['fontsize']) \n",
    "plt.rc('ytick', labelsize=style_dict['tick']['fontsize']) \n",
    "plt.rcParams.update({'figure.max_open_warning': 5})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bot = Bot(user_credentials='./JFGS.json')\n",
    "bot.send_message(text=\"{}\\nHello, John. Initiating sparsity analysis for the {} dataset on the {} model [{}/{}]\".format('-'*20, dataset_tag, model_tag, partition, nodes)) if bot_alive else ''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connecting to Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "username = 'postgres'\n",
    "password = 'admin'\n",
    "dbname = 'RecSys'\n",
    "hostname = 'localhost:5432'\n",
    "conn = db.get_database_connection(username, password, hostname, dbname)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 14 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "df_ratings = db.get_dataset_from_sparsity(data_path=None, conn=conn, dataset_tag=dataset_tag) \n",
    "df_ratings[['feedback_value']] = df_ratings[['feedback_value']].apply(pd.to_numeric)\n",
    "df_ratings.drop(['user', 'item', 'timestamp'], axis=1, inplace=True)\n",
    "df_ratings.columns = ['feedback_value', 'user', 'item']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1149780, 3)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feedback_value</th>\n",
       "      <th>user</th>\n",
       "      <th>item</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>3909265</td>\n",
       "      <td>534021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>3909270</td>\n",
       "      <td>558404</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>3909271</td>\n",
       "      <td>502082</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>3909271</td>\n",
       "      <td>512382</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>3909271</td>\n",
       "      <td>574144</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   feedback_value     user    item\n",
       "0               0  3909265  534021\n",
       "1               0  3909270  558404\n",
       "2               5  3909271  502082\n",
       "3               0  3909271  512382\n",
       "4               0  3909271  574144"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print (df_ratings.shape)\n",
    "df_ratings.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: 804846 \n",
      "Test size: 344934\n"
     ]
    }
   ],
   "source": [
    "df_train, df_test = train_test_split(df_ratings, test_size=0.3, random_state=random_state)\n",
    "print (\"Train size: {} \\nTest size: {}\".format(df_train.shape[0], df_test.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MostPopular(train_file=df_train, test_file=df_test, rank_length=rank_length, as_binary=False, verbose = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "model.compute(verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr_k = np.arange(1, rank_length+1, 1)\n",
    "model.evaluate(metrics=evaluation_metrics, n_ranks = arr_k)\n",
    "joblib.dump(model.evaluation_results, os.path.join(variables_output_folder, 'evaluation_results.joblib'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting Evaluations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation_results = joblib.load(os.path.join(variables_output_folder, 'evaluation_results.joblib'))\n",
    "fig, ax = plt.subplots(nrows=1, ncols=1, figsize=(10,10))\n",
    "\n",
    "for evaluation_name in [m + '@' for m in evaluation_metrics]:\n",
    "    ax.plot(arr_k, [evaluation_results[evaluation_name + str(x)] for x in arr_k], label=evaluation_name + 'k', color=style_dict['evaluations'][evaluation_name + 'k'])\n",
    "\n",
    "ax.set_xticks(np.arange(0, np.max(arr_k)+1, 2), minor=False)\n",
    "ax.set_xlabel('Rank k', fontsize = style_dict['label']['fontsize'])\n",
    "ax.tick_params(axis='both', which='major', labelsize=style_dict['tick']['fontsize'])\n",
    "ax.legend()\n",
    "ax.grid(True)\n",
    "filepath = os.path.join(figures_output_folder, 'item_rec_metrics.png')\n",
    "plt.savefig(filepath, bbox_inches = 'tight')\n",
    "bot.send_message(filePath=filepath) if bot_alive else ''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating on Sparsity Cenarios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_info = pd.read_sql(con=conn , sql=\"select * from datasets.dataset where version='{}'\".format(dataset_tag))\n",
    "df_sparsity_cenario = pd.read_sql(con=conn , sql=\"select * from sparsity.cenario\")\n",
    "sql_str = \"select id_user, id_item from sparsity.get_dataset_from_sparsity('{}', 1.0, 1.0)\".format(dataset_tag)\n",
    "df_whole = pd.read_sql(con=conn, sql=sql_str)\n",
    "print (\"Dataset {} (id_dataset = {}) with {} registers\".format(dataset_info['name'][0], dataset_info['id_dataset'][0], df_whole.shape[0]))\n",
    "print (\"Number of sparsity cenarios: \", df_sparsity_cenario.shape[0])\n",
    "df_sparsity_cenario.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sql_str = \"\"\"select * from sparsity.get_dataset_uss('{}')\"\"\".format(dataset_tag)\n",
    "df_users_uss = pd.read_sql(con=conn, sql=sql_str)\n",
    "df_users_uss.sort_values(['uss'], ascending=False).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sql_str = \"\"\"select * from sparsity.get_dataset_iss('{}')\"\"\".format(dataset_tag)\n",
    "df_items_iss = pd.read_sql(con=conn, sql=sql_str)\n",
    "df_items_iss.sort_values(['iss'], ascending=False).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uss_limits = df_sparsity_cenario['uss'].unique()\n",
    "iss_limits = df_sparsity_cenario['iss'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ratings.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_overall_sparsity = pd.read_csv(os.path.join('.', 'Outputs', dataset_tag, 'OS', 'Variables', 'df_overall_sparsity.tsv'), sep = '\\t', header = 0)\n",
    "if partition is not None:\n",
    "    df_overall_sparsity = partition_dataframe(df=df_overall_sparsity, nodes=nodes, sort_by='os', ascending=False)[partition-1].reset_index(drop = True)\n",
    "print (\"Input sparsity cenario dataframe shape: \", df_overall_sparsity.shape)\n",
    "df_overall_sparsity.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_evaluation_results(model, df_train, df_test, rank_length, as_binary=True):\n",
    "    model = MostPopular(train_file=df_train, test_file=df_test, rank_length=rank_length, as_binary=as_binary)\n",
    "    model.compute(verbose=False)\n",
    "    model.evaluate(metrics=evaluation_metrics, n_ranks = arr_k, verbose=False)\n",
    "    evaluation_results = pd.DataFrame.from_dict(data=model.evaluation_results, orient='index').T\n",
    "    evaluation_results['iss'] = [cenario['iss_limit']]\n",
    "    evaluation_results['uss'] = [cenario['uss_limit']]\n",
    "    return evaluation_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "main_analysis"
    ]
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "write_log(filepath=os.path.join(variables_output_folder, 'log.txt'), \n",
    "          mode=\"w+\", \n",
    "          text='[{}]\\tUSS \\tISS \\tIndex\\tPerc\\tFolder\\n'.format(time.strftime(\"%Y-%m-%d %H:%M:%S\", time.gmtime())))    \n",
    "\n",
    "arr_k = np.arange(1, rank_length+1, 1)\n",
    "arr_df_eval_metadata = np.repeat(None, 1) if n_folds == None else np.repeat(None, n_folds)\n",
    "first_value = [True] if n_folds == None else np.repeat(True, n_folds)\n",
    "kf = None if n_folds == None else KFold(n_splits=n_folds, random_state=random_state) \n",
    "# for uss_index, uss_limit in enumerate(uss_limits):\n",
    "#     for iss_limit in iss_limits:   \n",
    "for index, cenario in df_overall_sparsity.iterrows():\n",
    "    clear_output()\n",
    "    progbar.update_progress(index/float(df_overall_sparsity.shape[0]))   \n",
    "\n",
    "    start_time = time.time()\n",
    "    df_ratings = db.get_dataset_from_sparsity(conn=conn, dataset_tag=dataset_tag,                                               \n",
    "                                              iss=cenario['iss_limit'], \n",
    "                                              uss=cenario['uss_limit'])\n",
    "\n",
    "    df_ratings.drop(['user', 'item', 'timestamp'], axis=1, inplace=True)    \n",
    "    df_ratings[['feedback_value']] = df_ratings[['feedback_value']].apply(pd.to_numeric)\n",
    "    df_ratings.columns = ['feedback_value', 'user', 'item']    \n",
    "    \n",
    "    if n_folds == None:\n",
    "        df_train, df_test = train_test_split(df_ratings, test_size=0.3, random_state=random_state)\n",
    "        evaluation_results = get_evaluation_results(model, df_train, df_test, rank_length, as_binary=True)            \n",
    "        elapsed_time = time.time() - start_time\n",
    "        evaluation_results['elapsed_time'] = [elapsed_time]\n",
    "        \n",
    "        if (first_value[0]):            \n",
    "            first_value[0] = False\n",
    "            arr_df_eval_metadata[0] = evaluation_results.copy()    \n",
    "        else:\n",
    "            arr_df_eval_metadata[0] = arr_df_eval_metadata[0].append(evaluation_results)    \n",
    "        write_log(filepath=os.path.join(variables_output_folder, 'log.txt'), \n",
    "                  text='[{}]\\t{:.02f}\\t{:.02f}\\t{}/{}\\t{:.02f}%\\t{}/{}\\n'.format(time.strftime(\"%Y-%m-%d %H:%M:%S\", time.gmtime()), \n",
    "                                                                                 cenario['uss_limit'], \n",
    "                                                                                 cenario['iss_limit'], \n",
    "                                                                                 index+1, \n",
    "                                                                                 df_overall_sparsity.shape[0], \n",
    "                                                                                 100*(index/float(df_overall_sparsity.shape[0])), \n",
    "                                                                                 1, \n",
    "                                                                                 1)\n",
    "                 )\n",
    "    else:\n",
    "        \n",
    "        index_folder = 1\n",
    "        # Tentar fazer um shuffle e voltar com essa estrategia\n",
    "#         for train_index, test_index in kf.split(df_ratings):              \n",
    "#             df_train, df_test = df_ratings.iloc[train_index], df_ratings.iloc[test_index]            \n",
    "\n",
    "        for index_folder in np.arange(1, n_folds+1, 1):\n",
    "            df_train, df_test = train_test_split(df_ratings, test_size=0.3, random_state=random_state+10*index_folder-1)\n",
    "            \n",
    "            print (\"Processing folder {}/{}...\\n\".format(index_folder, n_folds))\n",
    "            evaluation_results = get_evaluation_results(model, df_train, df_test, rank_length, as_binary=True)            \n",
    "            elapsed_time = time.time() - start_time\n",
    "            evaluation_results['elapsed_time'] = [elapsed_time]\n",
    "\n",
    "            if (first_value[index_folder-1]):            \n",
    "                first_value[index_folder-1] = False\n",
    "    #             df_eval_metadata = evaluation_results.copy()\n",
    "                arr_df_eval_metadata[index_folder-1] = evaluation_results.copy()\n",
    "            else:\n",
    "                arr_df_eval_metadata[index_folder-1] = arr_df_eval_metadata[index_folder-1].append(evaluation_results)    \n",
    "            write_log(filepath=os.path.join(variables_output_folder, 'log.txt'), \n",
    "                      text='[{}]\\t{:.02f}\\t{:.02f}\\t{}/{}\\t{:.02f}%\\t{}/{}\\n'.format(time.strftime(\"%Y-%m-%d %H:%M:%S\", time.gmtime()), \n",
    "                                                                                     cenario['uss_limit'], \n",
    "                                                                                     cenario['iss_limit'], \n",
    "                                                                                     index+1, \n",
    "                                                                                     df_overall_sparsity.shape[0], \n",
    "                                                                                     100*(index/float(df_overall_sparsity.shape[0])), \n",
    "                                                                                     index_folder, \n",
    "                                                                                     n_folds)\n",
    "                     )\n",
    "            index_folder += 1\n",
    "\n",
    "for index, df_eval_metadata in enumerate(arr_df_eval_metadata):\n",
    "    folder_name = '1_1' if n_folds == None else str(n_folds) + '_' + str(index+1)\n",
    "    df_eval_metadata.reset_index(drop = True, inplace=True)\n",
    "    df_eval_metadata.to_csv(os.path.join(variables_output_folder, 'df_eval_metadata_fold{}.tsv'.format(folder_name)), sep = '\\t', header = True, index = False)\n",
    "text = \"Finished creating sparsity datasets [partition = {}] for\\t{} in\\t{}\".format(partition, dataset_tag, progbar.get_elapsed_time())\n",
    "bot.send_message(text=text) if bot_alive else ''       \n",
    "write_log(filepath=os.path.join(variables_output_folder, 'log.txt'), text=text)\n",
    "print (text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (\"Variables saved @ \", variables_output_folder)\n",
    "bot.send_message(text=\"End of analysis for the {} dataset\\n{}\".format(dataset_tag, '-'*20)) if bot_alive else ''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_________________"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_eval_metadata = pd.read_csv(os.path.join(variables_output_folder, 'df_eval_metadata.tsv'), sep = '\\t', header = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_eval_metadata.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# uss_limits = np.sort(np.array(df_eval_metadata['uss'].unique()))\n",
    "# iss_limits = np.sort(np.array(df_eval_metadata['iss'].unique()))\n",
    "# rank_lengths = np.arange(1, rank_length+1, 1) # Setting extra rank analysis\n",
    "\n",
    "# for rank in rank_lengths:\n",
    "#     arr_rank_metrics = [m + '@' + str(rank) for m in evaluation_metrics]\n",
    "#     for column in arr_rank_metrics:\n",
    "#         arr_metric = np.zeros([len(uss_limits), len(iss_limits)])\n",
    "#         for uss_index, uss_limit in enumerate(uss_limits):\n",
    "#             for iss_index, iss_limit in enumerate(iss_limits):            \n",
    "#                 arr_metric[uss_index, iss_index] = df_eval_metadata[(df_eval_metadata['uss'] == uss_limit) & (df_eval_metadata['iss'] == iss_limit)][column].reset_index(drop = True)[0]\n",
    "\n",
    "#         joblib.dump(arr_metric, os.path.join(variables_output_folder, 'arr_' + column.lower() + '_' + model_tag + '.joblib'))        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting Results from Sparsity Cenarios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cmapping = \"jet\"\n",
    "# tick_step = 5\n",
    "# figs = dict()\n",
    "# for metric in evaluation_metrics:\n",
    "#     figs[metric+'@'] = list()\n",
    "\n",
    "# for rank in rank_lengths:\n",
    "#     arr_rank_metrics = [m + '@' + str(rank) for m in evaluation_metrics]\n",
    "#     for column in arr_rank_metrics:\n",
    "#         arr_prec = joblib.load(os.path.join(variables_output_folder, 'arr_' + column.lower() + '_' + model_tag + '.joblib'))        \n",
    "\n",
    "#         fig, ax = plt.subplots(figsize=(style_dict['figure']['width'],style_dict['figure']['height']))    \n",
    "#         cax = plt.imshow(arr_prec, cmap=cmapping)\n",
    "#         plt.gca().invert_yaxis()\n",
    "#         cbar = plt.colorbar(cax, ticks = [x/100.0 for x in np.arange(0,1000,10)], shrink = 0.83)\n",
    "\n",
    "#         ax.set_xticklabels(uss_limits[0:len(uss_limits):tick_step])\n",
    "#         ax.set_yticklabels(iss_limits[0:len(uss_limits):tick_step])\n",
    "#         ax.set_xticks(np.arange(0, len(uss_limits), tick_step))    \n",
    "#         ax.set_yticks(np.arange(0, len(iss_limits), tick_step))    \n",
    "#         ax.set_xlabel('Last User Specific Sparsity', fontsize = style_dict['label']['fontsize'])\n",
    "#         ax.set_ylabel('Last Item Specific Sparsity', fontsize = style_dict['label']['fontsize'])    \n",
    "#         ax.tick_params(axis='both', which='major', labelsize=style_dict['tick']['fontsize'])\n",
    "#         cbar.set_label(column.title(), labelpad=-50,  y=1.08, rotation=0, fontsize = style_dict['label']['fontsize'])\n",
    "#         cbar.ax.tick_params(labelsize = style_dict['tick']['fontsize'])\n",
    "#         plt.clim(0, 1)\n",
    "#         plt.xticks(rotation = 'vertical')\n",
    "\n",
    "#         filename = '2d-' + column + '.png'       \n",
    "#         fig.savefig(os.path.join(figures_output_folder, filename), bbox_inches = 'tight')\n",
    "        \n",
    "#         fig.set_animated(True)\n",
    "#         figs[re.split(\"\\d\", column)[0]].append(fig)        \n",
    "#         if rank == rank_length: # Send only the target-analysis\n",
    "#             bot.send_message(filePath=os.path.join(figures_output_folder, filename)) if bot_alive else ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# for metric in evaluation_metrics:    \n",
    "#     filepaths = [os.path.join(figures_output_folder, '2d-' + column + '.png' ) for column in [metric + '@' + str(rank) for rank in rank_lengths]]\n",
    "#     output_filepath = os.path.join(figures_output_folder, '2d-' + metric + '@k' + '.gif' )\n",
    "#     create_gif(filepaths, output_filepath, duration=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from IPython import display\n",
    "# # display.HTML('<img src=\"{}\">'.format(output_filepath))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show_metric = lambda metric, k: figs[metric + '@'][int(k)-1]\n",
    "# interact(show_metric, k=widgets.IntSlider(min=1, max=rank_length, step=1, value=10), figs=figs, metric=evaluation_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__________"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
