{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "Traceback (most recent call last):\n  File \"g:\\google~2\\reposi~1\\projet~1\\worksp~2\\virtua~1\\casere~1\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 18, in swig_import_helper\n    fp, pathname, description = imp.find_module('_pywrap_tensorflow_internal', [dirname(__file__)])\n  File \"g:\\google~2\\reposi~1\\projet~1\\worksp~2\\virtua~1\\casere~1\\lib\\imp.py\", line 296, in find_module\n    raise ImportError(_ERR_MSG.format(name), name=name)\nImportError: No module named '_pywrap_tensorflow_internal'\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"g:\\google~2\\reposi~1\\projet~1\\worksp~2\\virtua~1\\casere~1\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 58, in <module>\n    from tensorflow.python.pywrap_tensorflow_internal import *\n  File \"g:\\google~2\\reposi~1\\projet~1\\worksp~2\\virtua~1\\casere~1\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 28, in <module>\n    _pywrap_tensorflow_internal = swig_import_helper()\n  File \"g:\\google~2\\reposi~1\\projet~1\\worksp~2\\virtua~1\\casere~1\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 20, in swig_import_helper\n    import _pywrap_tensorflow_internal\nModuleNotFoundError: No module named '_pywrap_tensorflow_internal'\n\n\nFailed to load the native TensorFlow runtime.\n\nSee https://www.tensorflow.org/install/errors\n\nfor some common reasons and solutions.  Include the entire stack trace\nabove this error message when asking for help.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32mg:\\google~2\\reposi~1\\projet~1\\worksp~2\\virtua~1\\casere~1\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\u001b[0m in \u001b[0;36mswig_import_helper\u001b[1;34m()\u001b[0m\n\u001b[0;32m     17\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 18\u001b[1;33m             \u001b[0mfp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpathname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdescription\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mimp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind_module\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'_pywrap_tensorflow_internal'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mdirname\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m__file__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     19\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mImportError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mg:\\google~2\\reposi~1\\projet~1\\worksp~2\\virtua~1\\casere~1\\lib\\imp.py\u001b[0m in \u001b[0;36mfind_module\u001b[1;34m(name, path)\u001b[0m\n\u001b[0;32m    295\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 296\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0mImportError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_ERR_MSG\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    297\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mImportError\u001b[0m: No module named '_pywrap_tensorflow_internal'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32mg:\\google~2\\reposi~1\\projet~1\\worksp~2\\virtua~1\\casere~1\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     57\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 58\u001b[1;33m   \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpywrap_tensorflow_internal\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     59\u001b[0m   \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpywrap_tensorflow_internal\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0m__version__\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mg:\\google~2\\reposi~1\\projet~1\\worksp~2\\virtua~1\\casere~1\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     27\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0m_mod\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 28\u001b[1;33m     \u001b[0m_pywrap_tensorflow_internal\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mswig_import_helper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     29\u001b[0m     \u001b[1;32mdel\u001b[0m \u001b[0mswig_import_helper\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mg:\\google~2\\reposi~1\\projet~1\\worksp~2\\virtua~1\\casere~1\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\u001b[0m in \u001b[0;36mswig_import_helper\u001b[1;34m()\u001b[0m\n\u001b[0;32m     19\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mImportError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 20\u001b[1;33m             \u001b[1;32mimport\u001b[0m \u001b[0m_pywrap_tensorflow_internal\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     21\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0m_pywrap_tensorflow_internal\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named '_pywrap_tensorflow_internal'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-2bd2d4124513>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_line_magic\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'matplotlib'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'inline'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mg:\\google~2\\reposi~1\\projet~1\\worksp~2\\virtua~1\\casere~1\\lib\\site-packages\\tensorflow\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[1;31m# pylint: disable=g-bad-import-order\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 24\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpywrap_tensorflow\u001b[0m  \u001b[1;31m# pylint: disable=unused-import\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     25\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mg:\\google~2\\reposi~1\\projet~1\\worksp~2\\virtua~1\\casere~1\\lib\\site-packages\\tensorflow\\python\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     47\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     48\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 49\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpywrap_tensorflow\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     50\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     51\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtools\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcomponent_api_helper\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mg:\\google~2\\reposi~1\\projet~1\\worksp~2\\virtua~1\\casere~1\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     72\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0msome\u001b[0m \u001b[0mcommon\u001b[0m \u001b[0mreasons\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0msolutions\u001b[0m\u001b[1;33m.\u001b[0m  \u001b[0mInclude\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mentire\u001b[0m \u001b[0mstack\u001b[0m \u001b[0mtrace\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     73\u001b[0m above this error message when asking for help.\"\"\" % traceback.format_exc()\n\u001b[1;32m---> 74\u001b[1;33m   \u001b[1;32mraise\u001b[0m \u001b[0mImportError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     75\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     76\u001b[0m \u001b[1;31m# pylint: enable=wildcard-import,g-import-not-at-top,unused-import,line-too-long\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mImportError\u001b[0m: Traceback (most recent call last):\n  File \"g:\\google~2\\reposi~1\\projet~1\\worksp~2\\virtua~1\\casere~1\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 18, in swig_import_helper\n    fp, pathname, description = imp.find_module('_pywrap_tensorflow_internal', [dirname(__file__)])\n  File \"g:\\google~2\\reposi~1\\projet~1\\worksp~2\\virtua~1\\casere~1\\lib\\imp.py\", line 296, in find_module\n    raise ImportError(_ERR_MSG.format(name), name=name)\nImportError: No module named '_pywrap_tensorflow_internal'\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"g:\\google~2\\reposi~1\\projet~1\\worksp~2\\virtua~1\\casere~1\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 58, in <module>\n    from tensorflow.python.pywrap_tensorflow_internal import *\n  File \"g:\\google~2\\reposi~1\\projet~1\\worksp~2\\virtua~1\\casere~1\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 28, in <module>\n    _pywrap_tensorflow_internal = swig_import_helper()\n  File \"g:\\google~2\\reposi~1\\projet~1\\worksp~2\\virtua~1\\casere~1\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 20, in swig_import_helper\n    import _pywrap_tensorflow_internal\nModuleNotFoundError: No module named '_pywrap_tensorflow_internal'\n\n\nFailed to load the native TensorFlow runtime.\n\nSee https://www.tensorflow.org/install/errors\n\nfor some common reasons and solutions.  Include the entire stack trace\nabove this error message when asking for help."
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import sys\n",
    "\n",
    "lib_path = './../Sources/Utilities'\n",
    "if (lib_path not in sys.path):\n",
    "    sys.path.append(lib_path) #src directory\n",
    "\n",
    "from messaging.print_functions import ProgressBar\n",
    "from messaging.telegrambot import Bot\n",
    "bot = Bot(user_credentials='./JFGS.json')\n",
    "\n",
    "# Checking if bot is ok\n",
    "bot.send_message(text=\"Hello, John\")\n",
    "progbar = ProgressBar(bar_length=20, bar_fill='#', elapsed_time=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# udata = pd.read_csv('ml-100k/u.data', sep='\\t', names=['userid', 'itemid', 'rating', 'timestamp'])\n",
    "# udata.tail()\n",
    "# hidden_dim=100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Amazon', 'BookCrossing', 'Jester', 'LastFM', 'MovieLens', 'Netflix']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os \n",
    "os.listdir('./../Datasets/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ardiya/anaconda3/lib/python3.5/site-packages/ipykernel/__main__.py:1: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "  if __name__ == '__main__':\n"
     ]
    }
   ],
   "source": [
    "udata = pd.read_csv('ml-1m/ratings.dat', sep='::', names=['userid', 'itemid', 'rating', 'timestamp'])\n",
    "udata.tail()\n",
    "hidden_dim=1200\n",
    "udata = udata[udata.rating >=5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 6014 users and 3232 items\n"
     ]
    }
   ],
   "source": [
    "map_uid_index = {uid:idx for idx, uid in enumerate(udata['userid'].unique())}\n",
    "map_iid_index = {iid:idx for idx, iid in enumerate(udata['itemid'].unique())}\n",
    "\n",
    "total_user = len(map_uid_index.items())\n",
    "total_item = len(map_iid_index.items())\n",
    "\n",
    "list_users = range(total_user)\n",
    "\n",
    "print(\"There are %d users and %d items\" % (total_user, total_item))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Split the dataset into 10% test set, the rest is splitted to 20% validation set and 10% train set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# m = np.mean(udata.rating)\n",
    "# s = np.std(udata.rating)\n",
    "# udata.rating -=  m\n",
    "# udata.rating /= s\n",
    "# udata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6014, 3232)\n",
      "(6014, 3232)\n",
      "(6014, 3232)\n",
      "(6014, 3232)\n"
     ]
    }
   ],
   "source": [
    "# normalize = lambda x:(x-3)/2\n",
    "# normalize = lambda x:x/5\n",
    "# normalize = lambda x: x\n",
    "normalize = lambda x: 1. if x == 5 else .5 if x == 4 else 0\n",
    "\n",
    "from sklearn import cross_validation\n",
    "udata_train_validation, udata_test = cross_validation.train_test_split(udata, test_size=0.2)\n",
    "udata_train, udata_validation = cross_validation.train_test_split(udata_train_validation, test_size=0.1)\n",
    "\n",
    "train_matrix = np.zeros([total_user, total_item], dtype=float)\n",
    "validation_matrix = np.zeros([total_user, total_item], dtype=float)\n",
    "test_matrix = np.zeros([total_user, total_item], dtype=float)\n",
    "full_matrix = np.zeros([total_user, total_item], dtype=float)\n",
    "\n",
    "data_map = [(train_matrix, udata_train), (validation_matrix, udata_validation),\n",
    "            (test_matrix, udata_test), (full_matrix, udata)]\n",
    "\n",
    "for imatrix, idata in data_map:\n",
    "    for line in idata.itertuples():\n",
    "        imatrix[map_uid_index[line[1]],\n",
    "                     map_iid_index[line[2]]] = normalize(line[3])\n",
    "\n",
    "print(train_matrix.shape)\n",
    "print(validation_matrix.shape)\n",
    "print(test_matrix.shape)\n",
    "print(full_matrix.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50]\n"
     ]
    }
   ],
   "source": [
    "def five_star_item(list2d):\n",
    "    ret = list()\n",
    "    for i in range(len(list2d)):\n",
    "        list1d = list2d[i]\n",
    "        ret.append([idx for idx, val in enumerate(list1d) if val == 1])\n",
    "    return ret\n",
    "m = five_star_item(full_matrix)\n",
    "print(m[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Randomly selecting item for each batch\n",
    "def get_batch(X, X_, size):\n",
    "    a = np.random.choice(len(X), size, replace=False)\n",
    "    return X[a], X_[a], a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Root Mean Square Error for validation test\n",
    "def rmse(prediction, ground_truth):\n",
    "    prediction = prediction[ground_truth.nonzero()].flatten() \n",
    "    ground_truth = ground_truth[ground_truth.nonzero()].flatten()\n",
    "    r = tf.sqrt(tf.reduce_mean(tf.square(tf.sub(prediction, ground_truth))))\n",
    "    return sess.run(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apk(actual, predicted, k=10):\n",
    "    \"\"\"\n",
    "    Computes the average precision at k.\n",
    "    This function computes the average prescision at k between two lists of\n",
    "    items.\n",
    "    Parameters\n",
    "    ----------\n",
    "    actual : list\n",
    "             A list of elements that are to be predicted (order doesn't matter)\n",
    "    predicted : list\n",
    "                A list of predicted elements (order does matter)\n",
    "    k : int, optional\n",
    "        The maximum number of predicted elements\n",
    "    Returns\n",
    "    -------\n",
    "    score : double\n",
    "            The average precision at k over the input lists\n",
    "    \"\"\"\n",
    "    if len(predicted)>k:\n",
    "        predicted = predicted[:k]\n",
    "\n",
    "    score = 0.0\n",
    "    num_hits = 0.0\n",
    "\n",
    "    for i,p in enumerate(predicted):\n",
    "        if p in actual and p not in predicted[:i]:\n",
    "            num_hits += 1.0\n",
    "            score += num_hits / (i+1.0)\n",
    "\n",
    "#     if not actual:\n",
    "#         return 0.0\n",
    "\n",
    "    return score / min(len(actual), k)\n",
    "\n",
    "def mapk(actual, predicted, k=10):\n",
    "    \"\"\"\n",
    "    Computes the mean average precision at k.\n",
    "    This function computes the mean average prescision at k between two lists\n",
    "    of lists of items.\n",
    "    Parameters\n",
    "    ----------\n",
    "    actual : list\n",
    "             A list of lists of elements that are to be predicted \n",
    "             (order doesn't matter in the lists)\n",
    "    predicted : list\n",
    "                A list of lists of predicted elements\n",
    "                (order matters in the lists)\n",
    "    k : int, optional\n",
    "        The maximum number of predicted elements\n",
    "    Returns\n",
    "    -------\n",
    "    score : double\n",
    "            The mean average precision at k over the input lists\n",
    "    \"\"\"\n",
    "    return np.mean([apk(a,p,k) for a,p in zip(actual, predicted)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.25\n",
      "0.2\n",
      "0.685185185185\n"
     ]
    }
   ],
   "source": [
    "print(apk(range(1,6),[6,4,7,1,2], 2)) #.25\n",
    "print(apk(range(1,6),[1,1,1,1,1], 5)) #0.2\n",
    "print(mapk([[1,3,4],[1,2,4],[1,3]],\n",
    "                   [range(1,6),range(1,6),range(1,6)], 3)) #0.685185185185185"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------\n",
      "|   ID|          Train RMSE|     Validation RMSE|\n",
      "-------------------------------------------------\n",
      "|  100|           39.931038|            1.432937|\n",
      "|  200|           16.115826|            1.057468|\n",
      "|  300|            6.476654|            0.994073|\n",
      "|  400|            2.721665|            0.988922|\n",
      "|  500|            1.236815|            0.983032|\n",
      "|  600|            0.617621|            0.984814|\n",
      "|  700|            0.341492|            0.984331|\n",
      "|  800|            0.210893|            0.986308|\n",
      "|  900|            0.145748|            0.985183|\n",
      "| 1000|            0.112052|            0.984707|\n",
      "--- Training time: 29.86239242553711 seconds ---\n",
      "TEST RMSE:  0.984737\n",
      "0.0473894246758\n",
      "0.0330894579315\n",
      "0.0255237778517\n",
      "0.0206289491187\n",
      "0.0178885378561\n",
      "0.0158343494808\n",
      "0.0136388534463\n",
      "0.0122371493631\n",
      "0.0110312240805\n",
      "0.0103714290406\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "#output is the train matrix\n",
    "data_x_ = train_matrix\n",
    "#add noise to the input\n",
    "data_x = data_x_ + np.random.normal(0, 0.1, (len(data_x_), len(data_x_[0])))\n",
    "\n",
    "#Setup Hyper Parameters\n",
    "epoch = 1000\n",
    "input_dim = len(data_x[0])\n",
    "#hidden_dim = 100\n",
    "batch_size = 512\n",
    "\n",
    "#variable for plot\n",
    "plt_x = list()\n",
    "plt_y1 = list()\n",
    "plt_y2 = list()\n",
    "\n",
    "sess = tf.Session()\n",
    "\n",
    "#setup placeholder and variables\n",
    "x = tf.placeholder(dtype=tf.float32, shape=[None, input_dim], name='x')\n",
    "x_ = tf.placeholder(dtype=tf.float32, shape=[None, input_dim], name='x_')\n",
    "\n",
    "enc_w = tf.Variable(tf.truncated_normal([input_dim, hidden_dim], dtype=tf.float32))\n",
    "enc_b = tf.Variable(tf.truncated_normal([hidden_dim], dtype=tf.float32))\n",
    "dec_w = tf.transpose(enc_w)\n",
    "dec_b = tf.Variable(tf.truncated_normal([input_dim], dtype=tf.float32))\n",
    "\n",
    "#setup network\n",
    "encoded = tf.nn.relu(tf.matmul(x, enc_w) + enc_b, name='encoded')\n",
    "decoded = tf.nn.relu(tf.matmul(encoded, dec_w) + dec_b, name='decoded')\n",
    "\n",
    "#setup loss\n",
    "loss = tf.sqrt(tf.reduce_mean(tf.square(tf.sub(x_, decoded))))  \n",
    "loss += 5e-5 * (tf.nn.l2_loss(enc_b)+ tf.nn.l2_loss(dec_b)+ tf.nn.l2_loss(enc_w))\n",
    "\n",
    "#optimizer\n",
    "global_step = tf.Variable(0, trainable=False)\n",
    "learning_rate = tf.train.exponential_decay(.01, global_step, 10000, 0.86, staircase=True)\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate).minimize(loss)\n",
    "\n",
    "sess.run(tf.initialize_all_variables())\n",
    "\n",
    "print(\"-\"*49)\n",
    "print(\"|   ID|%20s|%20s|\"%(\"Train RMSE\", \"Validation RMSE\"))\n",
    "print(\"-\"*49)\n",
    "for i in range(epoch):\n",
    "    b_x, b_x_, users = get_batch(data_x, data_x_, batch_size)\n",
    "    #update using optimizer\n",
    "    sess.run(optimizer, feed_dict={x:b_x, x_: b_x_})\n",
    "    #print loss\n",
    "    if (i+1) % 100 == 0:\n",
    "        #training loss\n",
    "        l = sess.run(loss, feed_dict={x:data_x, x_: data_x_})\n",
    "        \n",
    "        #prediction\n",
    "        pred = sess.run(decoded, feed_dict={x:data_x_, x_: data_x_})\n",
    "        val_rmse = rmse(pred, validation_matrix)\n",
    "        print(\"|%5d|%20f|%20f|\"%(i+1, l, val_rmse))\n",
    "        \n",
    "        #plot\n",
    "        plt_x.append(i+1)\n",
    "        plt_y1.append(l)\n",
    "        plt_y2.append(val_rmse)\n",
    "        \n",
    "\n",
    "print(\"--- Training time: %s seconds ---\" % (time.time() - start_time))\n",
    "print(\"TEST RMSE: \", rmse(pred, test_matrix))\n",
    "\n",
    "pred = sess.run(decoded, feed_dict={x:data_x_, x_: data_x_})\n",
    "pred = pred * (train_matrix == 0) # remove watched items from predictions\n",
    "pred = np.argsort(pred)\n",
    "\n",
    "for n in range(1, 11):\n",
    "    sr = mapk(actual=m, predicted=list(pred[:,-n:]), k=n)\n",
    "    print(\"MAP@\", n, \":\", sr)\n",
    "    #print(sr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[   0, 2147, 2148, ...,  169,  225,  181],\n",
       "       [   0, 2148, 2149, ...,  232,  484,  534],\n",
       "       [   0, 2149, 2150, ...,  534,  260, 1032],\n",
       "       ..., \n",
       "       [   0, 2148, 2149, ...,  534,  540,  260],\n",
       "       [   0, 2147, 2148, ..., 1074,  764, 1263],\n",
       "       [   0, 2147, 2148, ...,  584, 1400,  437]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Denoising AutoEncoder with normal distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------\n",
      "|   ID|          Train RMSE|     Validation RMSE|\n",
      "-------------------------------------------------\n",
      "|  100|           73.484512|            1.010365|\n",
      "|  200|           29.447542|            1.000000|\n",
      "|  300|           11.851405|            1.000000|\n",
      "|  400|            5.225925|            1.000000|\n",
      "|  500|            2.543041|            0.999939|\n",
      "|  600|            1.334266|            0.999838|\n",
      "|  700|            0.746467|            0.999522|\n",
      "|  800|            0.445319|            0.999893|\n",
      "|  900|            0.287127|            0.999504|\n",
      "| 1000|            0.203861|            0.997496|\n",
      "| 1100|            0.153237|            0.997192|\n",
      "| 1200|            0.126613|            0.995846|\n",
      "| 1300|            0.110805|            0.994437|\n",
      "| 1400|            0.101539|            0.992421|\n",
      "| 1500|            0.096193|            0.992462|\n",
      "| 1600|            0.092927|            0.990431|\n",
      "| 1700|            0.091095|            0.990898|\n",
      "| 1800|            0.089528|            0.990188|\n",
      "| 1900|            0.088718|            0.990272|\n",
      "| 2000|            0.088058|            0.988755|\n",
      "--- Training time: 64.73367118835449 seconds ---\n",
      "TEST RMSE:  0.988611\n",
      "0.059361489857\n",
      "0.0690056534752\n",
      "0.0524147359864\n",
      "0.0404992517459\n",
      "0.0341907770757\n",
      "0.0274874367217\n",
      "0.0238270000385\n",
      "0.0209536120179\n",
      "0.0184827627364\n",
      "0.0158574866841\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "#output is the train matrix\n",
    "data_x_ = train_matrix\n",
    "#add noise to the input\n",
    "data_x = data_x_ + np.random.normal(0, 0.2, (len(data_x_), len(data_x_[0])))\n",
    "\n",
    "#Setup Hyper Parameters\n",
    "epoch = 2000\n",
    "input_dim = len(data_x[0])\n",
    "#hidden_dim = 100\n",
    "batch_size = 512\n",
    "\n",
    "#variable for plot\n",
    "plt_x = list()\n",
    "plt_y1 = list()\n",
    "plt_y2 = list()\n",
    "\n",
    "sess = tf.Session()\n",
    "\n",
    "#setup placeholder and variables\n",
    "x = tf.placeholder(dtype=tf.float32, shape=[None, input_dim], name='x')\n",
    "x_ = tf.placeholder(dtype=tf.float32, shape=[None, input_dim], name='x_')\n",
    "train_inputs = tf.placeholder(tf.int32, shape=[None], name='train_inputs')\n",
    "\n",
    "enc_w = tf.Variable(tf.truncated_normal([input_dim, hidden_dim], dtype=tf.float32))\n",
    "enc_b = tf.Variable(tf.truncated_normal([hidden_dim], dtype=tf.float32))\n",
    "#dec_w = tf.transpose(enc_w)\n",
    "dec_w = tf.Variable(tf.truncated_normal([hidden_dim, input_dim], dtype=tf.float32))\n",
    "dec_b = tf.Variable(tf.truncated_normal([input_dim], dtype=tf.float32))\n",
    "embeddings = tf.Variable(tf.random_uniform([input_dim, hidden_dim], -.5, .5))\n",
    "\n",
    "#setup network\n",
    "encoded = tf.nn.relu(tf.matmul(x, enc_w) + enc_b, name='encoded')\n",
    "embed = tf.nn.embedding_lookup(embeddings, train_inputs, name='embed')\n",
    "enc_embed = tf.add(encoded, embed)\n",
    "decoded = tf.nn.relu(tf.matmul(enc_embed, dec_w) + dec_b, name='decoded')\n",
    "\n",
    "#setup loss\n",
    "loss = tf.sqrt(tf.reduce_mean(tf.square(tf.sub(x_, decoded))))  \n",
    "loss += 5e-5 * (\n",
    "        tf.nn.l2_loss(embeddings)\n",
    "        + tf.nn.l2_loss(enc_b)\n",
    "        + tf.nn.l2_loss(dec_b)\n",
    "        + tf.nn.l2_loss(enc_w)\n",
    "        + tf.nn.l2_loss(dec_w)\n",
    ")\n",
    "\n",
    "#optimizer\n",
    "global_step = tf.Variable(0, trainable=False)\n",
    "learning_rate = tf.train.exponential_decay(.01, global_step, 10000, 0.86, staircase=True)\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate).minimize(loss)\n",
    "\n",
    "sess.run(tf.initialize_all_variables())\n",
    "\n",
    "print(\"-\"*49)\n",
    "print(\"|   ID|%20s|%20s|\"%(\"Train RMSE\", \"Validation RMSE\"))\n",
    "print(\"-\"*49)\n",
    "for i in range(epoch):\n",
    "    b_x, b_x_, users = get_batch(data_x, data_x_, batch_size)\n",
    "    #update using optimizer\n",
    "    sess.run(optimizer, feed_dict={x:b_x, x_: b_x_, train_inputs:users})\n",
    "    #print loss\n",
    "    if (i+1) % 100 == 0:\n",
    "        #training loss\n",
    "        l = sess.run(loss, feed_dict={x:data_x, x_: data_x_, train_inputs:list_users})\n",
    "        \n",
    "        #prediction\n",
    "        pred = sess.run(decoded, feed_dict={x:data_x_, train_inputs:list_users})\n",
    "        val_rmse = rmse(pred, validation_matrix)\n",
    "        print(\"|%5d|%20f|%20f|\"%(i+1, l, val_rmse))\n",
    "        \n",
    "        #plot\n",
    "        plt_x.append(i+1)\n",
    "        plt_y1.append(l)\n",
    "        plt_y2.append(val_rmse)\n",
    "        \n",
    "\n",
    "print(\"--- Training time: %s seconds ---\" % (time.time() - start_time))\n",
    "print(\"TEST RMSE: \", rmse(pred, test_matrix))\n",
    "\n",
    "pred = sess.run(decoded, feed_dict={x:data_x_, train_inputs:list_users})\n",
    "pred = pred * (train_matrix == 0) # remove watched items from predictions\n",
    "pred = np.argsort(pred)\n",
    "\n",
    "for n in range(1, 11):\n",
    "    sr = mapk(actual=m, predicted=list(pred[:,-n:]), k=n)\n",
    "    print(\"MAP@\", n, \":\", sr)\n",
    "    #print(sr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAEACAYAAAC9Gb03AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAIABJREFUeJzt3Xl0VPX9//HnOwlBIkkIS4IIjbiAC6gsLojUURC+ULZW\nWQWxi/X0tF+R77EK/UkJ3axW+23t0Spt1cgiLv2qKETAwoi0IijgiiDVIosEhEBYlBjy+f1xJzEr\nmZCZuZPJ63HOnNy587n3vpmQ9/3M+37mc805h4iIJK4kvwMQEZHoUqIXEUlwSvQiIglOiV5EJMEp\n0YuIJDglehGRBBdWojezaWb2npm9Y2bzzSzVzLLMbJmZbTazpWaWGe1gRUSk4epN9GbWCfhvoLdz\n7kIgBZgATAdecc51B1YAM6IZqIiInJxwSzfJwKlmlgK0AnYCo4D80Ov5wOjIhyciIo1Vb6J3zu0C\n7gc+xUvwB51zrwA5zrnCUJvdQHY0AxURkZMTTummDV7vPRfohNezvwGoPneC5lIQEYlDKWG0GQR8\n7JzbD2BmzwFXAIVmluOcKzSzjsCe2jY2M50AREROgnPOIrGfcGr0nwKXm9kpZmbAQOADYBFwU6jN\nFOCFunbgnIurx6xZs3yPoSnEFK9xKSbF1BziiqR6e/TOubVm9iywAfgq9HMOkA48bWbfA7YBYyMa\nmYiIREQ4pRucc7OB2dVW78cr64iISBxrlt+MDQQCfodQQzzGBPEZl2IKj2IKX7zGFSkW6VpQjQOY\nuWgfQ0Qk0ZgZLkIXY8Mq3YhIeM444wy2bdvmdxjShOTm5vKf//wnqsdQj14kgkK9ML/DkCakrv8z\nkezRN8savYhIc6JELyKS4JToRUQSnBK9iDRYWVkZ6enp7NixI6JtJTqU6EWagfT0dDIyMsjIyCA5\nOZm0tLSKdU8++WSD95eUlMShQ4fo3LlzRNs21MyZM0lNTSUjI4O2bdsyYMAA1q1bV/H6P/7xD5KS\nkhg3blyV7davX09SUhKDBw+uWPfcc89x8cUX06ZNG7Kzs7n22msrTk6Vj5ORkUF6ejrZ2U1nwl4l\nepFm4NChQxQXF1NcXExubi6LFy+uWDdhwoQa7Y8fP+5DlCdn0qRJFBcXs3fvXq688krGjBlT5fWc\nnBxWrVpFcXFxxbr8/Hy6d+9e8Xzz5s1873vf409/+hMHDhzgk08+4Uc/+hFJSUk1jlNcXMyhQ4fY\ns6fWeRzjkhK9SDNT26RZM2fOZPz48UycOJHMzEzmz5/PmjVr6NevH1lZWZx++ulMnTq14gRw/Phx\nkpKS+PTTTwGYPHkyU6dOZdiwYWRkZNC/f/+K7xM0pC1AQUEB3bt3Jysri1tvvZUrr7ySJ554ot5/\nV3JyMhMnTmT79u0cPHiwYv0pp5zCiBEjWLhwIQClpaU8++yzTJw4saLNxo0bOeeccxgwYAAAp556\nKt/5znfo1KlTg9/feKRELyIAPP/880yaNImDBw8ybtw4WrRowQMPPMD+/fv55z//ydKlS3nkkUcq\n2nuT2X7tySef5Ne//jVFRUV06dKFmTNnNrjtnj17GDduHPfffz+ff/45Xbt2rVKKOZFjx46Rn59P\nhw4dyMjIqHLsG2+8seJkUVBQQO/evauUXvr06cO7777L7bffTjAY5OjRo2G+a02DEr1IDJlF5hEN\nV155JcOGDQOgZcuW9OnTh0suuQQz44wzzuDmm2/m1VdfrWhf/VPB9ddfT69evUhOTuaGG25g48aN\nDW67ePFievXqxfDhw0lOTmbatGm0a9fuhHHPnz+ftm3bcuqppzJ37lyeffbZGieWK6+8kt27d/Px\nxx/zxBNPcOONN1Z5/eyzz2blypVs376dsWPH0r59e77//e/z5Zdf1jhO+WPIkCH1vaVxQ4leJIac\ni8wjGrp06VLl+ebNmxk+fDinnXYamZmZzJo1i88//7zO7Tt27FixnJaWxuHDhxvcdteuXTXiqO8i\n7g033MD+/fspLCyke/furF+/vtZ2kyZN4o9//COrV69m1KhRNV6//PLLeeqpp9izZw+vvvoqK1as\n4O67765xnPLH0qVLTxhXPFGiFxGgZnnllltuoWfPnnz88cccPHiQ2bNnR316h9NOO43t27dXWbdz\n586wtm3Xrh2PPPIId911F3v37q3x+uTJk3nwwQcZNWoUqampJ9zXJZdcwujRo3nvvffCDz6OxSTR\nFxbG4igiEkmHDh0iMzOTVq1asWnTpir1+WgZPnw4GzZsYPHixRw/fpw//OEPJ/wUUd15553HoEGD\n+N3vflfjtbPOOotXX32V2bOr31oDVq1axd/+9reKE8SmTZt48cUX6dev38n/Y+JITBL94sWxOIqI\nhKN6z70u999/P48//jgZGRn86Ec/Yvz48XXup759hts2Ozubp556imnTptG+fXs++eQTevXqRcuW\nLcOKGeD222/n4YcfZv/+/TVe69+/Pzk5OTXWZ2Vl8dxzz9GzZ08yMjIYPnw448eP53/+538q2syf\nP7/KOPqMjAyKiorCjstPMZm9cvRox3PPRfUwInFBs1dGVllZGZ06deLvf/87/fv39zucqEiY2Sv/\n8Q/44otYHElEmrqlS5dy8OBBjh07xi9+8QtSU1O59NJL/Q6rSas30ZtZNzPbYGbrQz8PmtmtZpZl\nZsvMbLOZLTWzzLr2cfHFsGJFZAMXkcS0evVqzjzzTHJycli+fDnPP/88LVq08DusJq1BpRszSwJ2\nAJcBPwH2OefuNbM7gSzn3PRatnG/+51j61Z4+OFIhS0Sn1S6kYaKx9LNIODfzrntwCggP7Q+Hxhd\n10YjR8KLL0Zv/K+IiNStoYl+HLAgtJzjnCsEcM7tBuqcyq1bN2jdGur4HoOIiERR2InezFoAI4Fn\nQquq989P2F8fORIWLWpYcCIi0ngpDWg7FHjLOVf+7YVCM8txzhWaWUegzjk78/Ly+PxzKCiAq68O\nEAgEGhGyiEjiCQaDBIPBqOw77IuxZvYk8LJzLj/0/B5gv3PunvouxjrnKC2Fjh1hwwaoNpWFSMLQ\nxVhpqLi5GGtmaXgXYv+v0up7gGvNbDMwEPjtifaRkgJDh3oXZUWkadm2bRtJSUmUlZUBMGzYMObO\nnRtW24a6++67+eEPf3jSsUpNYSV659xR51wH59yhSuv2O+cGOee6O+cGO+cO1Lef8tE3IhJbQ4cO\nJS8vr8b6F154gdNOOy2spFx56oIlS5YwefLksNqeyKuvvlpjtsoZM2YwZ86csLZviPz8fFJSUsjI\nyKBNmzb06tWLxZXmZyk/QfXp06fKdvv27SM1NZUzzzyzYt3q1avp378/bdq0oX379gwYMIC33nqr\nxnEqT5ewe/fuiP+bwhXT2SuHDIF//hMOHaq/rYhEzpQpU5g3b16N9fPmzWPy5MlVbpkXS865sE8K\nkXDFFVdQXFzMgQMHKubvqXyLQYCjR4/ywQcfVDxfsGABZ511VsXzQ4cOMWLECKZOnUpRURE7d+5k\n1qxZVebjKT9O+W0Hi4uLq0zNHGsx/e1mZMDll8Py5bE8qoiMHj2affv2sXr16op1Bw4c4KWXXqq4\nCceSJUvo3bs3mZmZ5Obm1jrLY7mrr76aRx99FPDmo7n99tvp0KEDZ599dpVeMsDjjz/O+eefT0ZG\nBmeffXZFb/3o0aMMGzaMXbt2Ven1zp49u8qnhUWLFtGjRw/atm3LNddcw4cffljxWteuXbn//vu5\n6KKLyMrKYsKECZSUlIT1nkyePJkjR47w0Ucf1Vj/+OOPVzyvfqOSLVu2YGaMHTsWM6Nly5YMGjSI\nHj16hHVcP8T8NK5hliKxd8oppzBmzJgq91596qmnOO+88yoSVOvWrZk7dy4HDx5k8eLFPPzwwywK\n4491zpw5LFmyhLfffps333yTZ599tsrrOTk5LFmyhOLiYh577DGmTZvGxo0bSUtLo6CggE6dOtXo\n9Zb38rds2cLEiRN54IEH2Lt3L0OHDmXEiBGUlpZW7P+ZZ55h2bJlfPLJJ7z99ttVknRdjh8/zqOP\nPkpqaiq5ubkV682MSZMmsXDhQpxzfPDBBxw5cqTKXDvdunUjOTmZm266iZdffpkDB+qtWvsu5ol+\nxAhv2uImdJN5kcjJy6v93oC11M/rbF9X23pMmTKFZ555pqLHO3fuXKZMmVLx+je/+U0uuOACAHr0\n6MH48eOr3DqwLs888wy33XYbnTp1ok2bNsyYMaPK60OHDuWMM84AYMCAAQwePJjXXnstrJiffvpp\nhg8fzjXXXENycjK33347X3zxBf/6178q2kydOpWcnBzatGnDiBEjqtzCsLrXX3+dtm3b0qpVK+64\n4w7mzZtH+/btq7Tp3Lkz5557LsuXL2fu3Lk1rkWkp6ezevVqkpKS+OEPf0h2djajRo2qcrOT8uO0\nbduWrKwszjnnnLD+vdES80SfmwudOsGaNbE+skgcyMur/d6AJ0r04batR//+/enQoQPPP/88H3/8\nMevWrWPixIkVr69du5ZrrrmG7Oxs2rRpwyOPPBLWTT+q3/6vcg8ZvJtx9+vXj3bt2pGVlUVBQUHY\nNxPZtWtXjR53ly5dqtx1qvL88vXdwrBfv37s37+fAwcOMHLkSFatWlVru/LyzcKFC2u96Ny9e3ce\nffRRPv30U9577z127drFbbfdVuM4+/fvp6ioqEZ5KNZ8uQKj8o2IPyZPnkx+fj7z5s1jyJAhdOjQ\noeK1iRMnMnr0aHbu3MmBAwe45ZZbwvpOQPXb/23btq1iuaSkhOuvv5477riDvXv3UlRUxNChQyv2\nW9+F2E6dOlXZH8D27dvrvY9sfdLS0njooYeYO3cub7/9do3Xr7vuOhYvXsxZZ51V77G6devGTTfd\nFNe3HfQl0Y8YoWGWIn648cYbeeWVV/jrX/9apWwDcPjwYbKysmjRogVr165lwYIFVV6vK+mPHTuW\nBx54gJ07d1JUVMQ999xT8VpJSQklJSW0b9+epKQkCgoKWLZsWcXrOTk57Nu3r8bIl8r7Xrx4MStX\nrqS0tJT77ruPU045JSK3+MvKyuLmm2+uctG5/N+YlpbGypUr+ctf/lJju82bN/P73/++4lPF9u3b\nefLJJ6vEFG9fmvMl0fftC0VFsHWrH0cXab5yc3O54oorOHr0KCNHjqzy2kMPPcTMmTPJzMzkV7/6\nFePGjavyel23A7z55psZMmQIF110EX379uW6666reK1169Y88MADjBkzhrZt27Jw4UJGjRpV8Xr3\n7t2ZMGECZ555Jm3btq0x1rxbt27MmzePn/zkJ3To0IHFixfz4osvkpKSUiOOkzF16lQKCgoqeuOV\n99e7d2+6du1aY5v09HTeeOMNLrvsMtLT07niiiu48MILue+++yrarFmzpsY4+vJx9n6Iya0EazvG\nzTfD+efDtGlRPbxITGkKBGmouJkCIRpUpxcRiQ3fevRHj3qTnG3bBllZUQ1BJGbUo5eGSugefVoa\nBALe1MUiIhI9viV6UPlGRCQWfCvdAHz2mXdBtrAQUlOjGoZITKh0Iw2V0KUbgNNO8+4nG+a3oUVE\n5CQ05FaCUVH+5amBA/2ORKTxcnNzYzrtrjR91aeMiAZfSzcA77wDo0fDv//tzdckIiIJVLoB6NkT\nysrg/ff9jkREJDH5nujNNPeNiEg0hXtz8Ewze8bMNpnZ+2Z2mZllmdkyM9tsZkvNLPNkg9AwSxGR\n6Am3R/9HYIlz7jzgIuBDYDrwinOuO7ACmHGC7U/oqqtg0yZvmKWIiERWvYnezDKAAc65xwCcc6XO\nuYPAKCA/1CwfGH2yQaSmwuDB3p2nREQkssLp0XcFPjezx8xsvZnNMbM0IMc5VwjgnNsNZDcmENXp\nRUSiI5xx9ClAb+DHzrk3zex/8co21cdM1jmGMq/Src8CgQCBQKBGm2HD4Mc/hi++gFatwohKRCSB\nBINBgsFgVPZd7zh6M8sBXnfOnRl6fiVeoj8LCDjnCs2sI7AyVMOvvv0Jx9FXdtVVcMcd8K1vNfBf\nISKSYGI6jj5UntluZt1CqwYC7wOLgJtC66YALzQ2GJVvREQiL6xvxprZRcBfgRbAx8B3gWTgaaAL\nsA0Y65w7UMu2Yffot2yBq6+GHTv0LVkRad4i2aP3fQqE6rp3hwULoE+fKAYlIhLnEmoKhOr05SkR\nkchSohcRSXBxV7opLfXuJbthA3TpEsXARETiWEKXblJSvDH1Gn0jIhIZcZfoQcMsRUQiKe5KNwDF\nxdC5M+zcCenpUQpMRCSOJXTpBiAjAy6/HJYv9zsSEZGmLy4TPWj0jYhIpMRl6QZg2zbo2xd274bk\n5CgEJiISxxK+dAOQmwudOsGaNX5HIiLStMVtogeVb0REIiGuE72GWYqINF5cJ/q+faGoCLZu9TsS\nEZGmK64TfVISDB+uXr2ISGPEdaIH1elFRBorbodXljt61JvkbNs2yMqKYGAiInGsWQyvLJeWBoEA\nFBT4HYmISNMU94keVL4REWmMuC/dAHz2GZx/PhQWQmpqhAITEYljMS/dmNl/zOxtM9tgZmtD67LM\nbJmZbTazpWaWGYmAanPaadCtG7z2WrSOICKSuMIt3ZQBAedcL+fcpaF104FXnHPdgRXAjGgEWG7k\nSA2zFBE5GeEmequl7SggP7ScD4yOVFC1GTHCq9NHudIkIpJwwk30DlhuZuvM7AehdTnOuUIA59xu\nIDsaAZbr2RPKyuD996N5FBGRxJMSZrv+zrnPzKwDsMzMNuMl/8rq7Gvn5eVVLAcCAQKBQAPDBLOv\n577p0aPBm4uIxLVgMEgwGIzKvhs86sbMZgGHgR/g1e0LzawjsNI5d14t7Rs96qbc8uXw85/D669H\nZHciInErpqNuzCzNzFqHlk8FBgPvAouAm0LNpgAvRCKgE7nqKti0yRtmKSIi4QmnRp8DrDazDcAa\n4EXn3DLgHuDaUBlnIPDb6IXpSU2FwYNh8eJoH0lEJHE0iS9MVTZvHvz97/DccxHbpYhI3Ilk6abJ\nJfp9+6BrV69806pVxHYrIhJXmtWkZtW1awe9esGKFX5HIiLSNDS5RA+6xaCISEM0udINwJYtcPXV\nsGOHN75eRCTRNOvSDXgTnKWnw/r1fkciIhL/mmSih6/nvhERkRNrsoleNyMREQlPk6zRA5SWeveS\n3bABunSJ+O5FRHzV7Gv0ACkpMGyYRt+IiNSnySZ60DBLEZFwNNnSDUBxMXTuDDt3eqNwREQShUo3\nIRkZ0K+fN32xiIjUrkknetAwSxGR+jTp0g3Atm3Qty/s3g3JyVE7jIhITKl0U0luLnTqBGvW+B2J\niEh8avKJHvTlKRGRE0mYRK9hliIitUuIRN+nDxQVwdatfkciIhJ/EiLRJyXpy1MiInUJO9GbWZKZ\nrTezRaHnWWa2zMw2m9lSM8uMXpj10zBLEZHaNaRHPxX4oNLz6cArzrnuwApgRiQDa6iBA+Gtt7wS\njoiIfC2sRG9mnYFhwF8rrR4F5IeW84HRkQ2tYdLSIBCAggI/oxARiT/h9uj/F/gpUPmbTznOuUIA\n59xuIDvCsTWYhlmKiNSUUl8DM/sWUOic22hmgRM0rfPrr3l5eRXLgUCAQOBEuzl5w4fDT38KJSWQ\nmhqVQ4iIREUwGCQYDEZl3/VOgWBmvwEmAaVAKyAdeA7oCwScc4Vm1hFY6Zw7r5btozoFQnWXXQa/\n+Y1XsxcRaapiOgWCc+5nzrlvOOfOBMYDK5xzk4EXgZtCzaYAL0QioMbSl6dERKpqzDj63wLXmtlm\nYGDoue/Kh1nG8EOEiEhca/KzV1bnHHTtCi+9BD16xOywIiIRpdkrT8BM5RsRkcoSLtGDviUrIlJZ\nwpVuwBtemZMDH37o/RQRaWpUuqlHaipcey0sXux3JCIi/kvIRA+q04uIlEvI0g3Avn3e6JvCQmjV\nKuaHFxFpFJVuwtCuHfTqBStW+B2JiIi/EjbRg8o3IiKQwKUbgC1b4OqrYccOb3y9iEhTodJNmLp1\ng/R0WL/e70hERPyT0Ike9OUpEZGET/S6GYmINHcJXaMHKC2Fjh1hwwbo0sW3MEREGkQ1+gZISYFh\nwzT6RkSar4RP9KBhliLSvCV86QaguBg6d4adO71ROCIi8U6lmwbKyIB+/WD5cr8jERGJvWaR6EHD\nLEWk+WoWpRuAbdugb1/YvRuSk/2ORkTkxGJaujGzlmb2hpltMLN3zWxWaH2WmS0zs81mttTMMiMR\nULTk5sLpp8OaNX5HIiISW/UmeufcMeBq51wv4GJgqJldCkwHXnHOdQdWADOiGmkEqHwjIs1RWDV6\n59zR0GJLIAVwwCggP7Q+Hxgd8egiTMMsRaQ5CivRm1mSmW0AdgPLnXPrgBznXCGAc243kB29MCOj\nTx8oKoKtW/2OREQkdlLCaeScKwN6mVkG8JyZXYDXq6/SrK7t8/LyKpYDgQCBQKDBgUZCUpJXvnnx\nRZg2zZcQRERqFQwGCQaDUdl3g0fdmNlM4CjwAyDgnCs0s47ASufcebW0j4tRN+VWrYJJk7ypi9u3\n9zsaEZHaxXrUTfvyETVm1gq4FtgELAJuCjWbArwQiYCi7ZvfhAkTYPJkKCvzOxoRkeirt0dvZj3x\nLrYmhR5POed+bWZtgaeBLsA2YKxz7kAt28dVjx7gq6+8O08NHw7Tp/sdjYhITZHs0TebL0xVt2MH\nXHIJPP00DBjgdzQiIlVprpsI6NwZHnsMJk6EPXv8jkZEJHqabY++3M9+Bm+9BQUF3qgcEZF4oB59\nBP3iF/Dll/Cb3/gdiYhIdDT7Hj3Arl3el6kWLPAu0oqI+E09+gjr1AmeeMIbX19Y6Hc0IiKRpUQf\ncu218P3vexdnjx/3OxoRkchRoq9k1ixwDn75S78jERGJHNXoq9m9G3r39ko5gwb5HY2INFeq0UdR\nx44wdy7ceCN89pnf0YiINJ4SfS0GDoRbbvHq9aWlfkcjItI4SvR1uOsuSEmB2bP9jkREpHFUoz+B\nwkJvfP2jj8LgwX5HIyLNiWr0MZKTA/Pnw5QpsHOn39GIiJwcJfp6XHUV/OQn3hz2qteLSFOk0k0Y\nyspg2DDo1QvuvtvvaESkOdB89D7Yu9cbX//II17SFxGJJiV6n7z2GowZA+vWQZcufkcjIolMF2N9\nMmAA3HYbjBvn3Y5QRKQpUI++gcrKYMQIuOACuPdev6MRkUQV0x69mXU2sxVm9r6ZvWtmt4bWZ5nZ\nMjPbbGZLzSwzEgHFu6QkyM+HhQvhpZf8jkZEpH719ujNrCPQ0Tm30cxaA28Bo4DvAvucc/ea2Z1A\nlnNuei3bJ1SPvty//gXf/jasXQu5uX5HIyKJJqY9eufcbufcxtDyYWAT0Bkv2eeHmuUDoyMRUFNx\nxRXw05969fqSEr+jERGpW4Nq9GZ2BhAEegDbnXNZlV7b75xrW8s2CdmjB2/u+lGj4Oyz4fe/9zsa\nEUkkkezRpzTgoK2BZ4GpzrnDZlY9e9eZzfPy8iqWA4EAgUCgYVHGKTN4/HFvfP03vwmjm9VnGhGJ\npGAwSDAYjMq+w+rRm1kK8BJQ4Jz7Y2jdJiDgnCsM1fFXOufOq2XbhO3Rl1uzBkaOhDfegK5d/Y5G\nRBKBH+PoHwU+KE/yIYuAm0LLU4AXIhFQU3T55TBjBowdC8eO+R2NiEhV4Yy66Q+sAt7FK8844GfA\nWuBpoAuwDRjrnDtQy/YJ36MHr17/ne9435h94AG/oxGRpk5TIMSpoiJv/vp774Xrr/c7GhFpypTo\n49i6dfCtb8Hrr8NZZ/kdjYg0VZrrJo5dcgnMnOnV67/80u9oRETUo48K57xEn50NDz7odzQi0hSp\nRx/nzOCvf4WXX4annvI7GhFp7tSjj6L162HIEG9enHPO8TsaEWlK1KNvInr3htmzvZuVfPGF39GI\nSHOlHn2UOefdWDwz07sNoYhIONSjb0LMYM4cWLkSFizwOxoRaY7Uo4+RjRvh2mu9+86ee67f0YhI\nvFOPvgm6+GL49a+9ev3Ro35HIyLNiXr0MeQcTJoErVp5wy9FROqiHn0TZQYPPwyrV8PcuX5HIyLN\nhXr0PnjnHRg4EF59Fc4/3+9oRCQeqUffxF14Ifz2t169/sgRv6MRkUSnHr1PnIMpUyApybsdoYhI\nZerRJwAz+POfYe1aeOwxv6MRkUSmHr3P3n8fAgHvC1U9evgdjYjEC/XoE8gFF8B998GwYd5InNJS\nvyMSkURTb6I3s7+ZWaGZvVNpXZaZLTOzzWa21MwyoxtmYpsyxavTz5njjcJRwheRSAqnR/8YMKTa\nuunAK8657sAKYEakA2turrkGVq3yxtnPmeP19OfNU8IXkcYLq0ZvZrnAi865C0PPPwSucs4VmllH\nIOicq3UGF9XoG845WLECZs2CvXu9WxNOmADJyX5HJiKxEg81+mznXCGAc243kB2JYMRj5n2h6rXX\n4KGHvF7++efD/Plw/Ljf0YlIUxOpi7HqskdB5YT/4INe0r/gAiV8EWmYlJPcrtDMciqVbvacqHFe\nXl7FciAQIBAInORhmyczGDTIS/r/+IdX0vnlL+HnP4dx41TSEUkEwWCQYDAYlX2HW6M/A69G3zP0\n/B5gv3PuHjO7E8hyzk2vY1vV6CPMOXjlFS/h79+vhC+SiCJZo6830ZvZAiAAtAMKgVnA88AzQBdg\nGzDWOXegju2V6KOkcsIvKvIS/tixSvgiiSCmib7RB1CijzrnYPlyL+EfOKCEL5IIlOilVkr4IolD\niV5OyDlYtgzy8uDgQS/hjxmjhC/SlCjRS1jKE/6sWVBcrIQv0pQo0UuDVE74hw55Cf/665XwReKZ\nEr2cFOdg6VKvpFOe8MeM8W5+IiLxRYleGqU84c+aBYcPez+vv14JXySeKNFLRDgHL7/s9fCPHPm6\npKOEL+I/JXqJqPKEP2sWHD3q/bzuOiV8ET8p0UtUOAcFBV4P/+hRuOsuGDwY2rb1OzKR5keJXqKq\nPOH//vfwxhuQnQ19+3qPPn2gd29o08bvKEUSmxK9xMzx47BlC7z5Jrz1lvdz40bo1OnrxN+3r5f8\n09P9jlaLeiGMAAAI10lEQVQkcSjRi69KS+HDD72kX34CeOcd+MY3qvb8e/WCU0/1O1qRpkmJXuLO\nV1/BBx9U7fm/9x6ceWbVnv9FF0Famt/RisQ/JXppEkpKvGRfOflv2gTnnFM1+V94IZxyit/RisQX\nJXppsr78Et59t2ry37IFzj23avLv2RNSU/2OVsQ/SvSSUL74At5+u2ry//e/vfvjVk7+F1wALVr4\nHa1IbCjRS8I7csQb3VM5+W/bBj16QNeukJPz9aNjx6+Xs7OhZUu/oxdpPCV6aZYOHfKS//btsHs3\nFBbWfOzZA61bV03+tZ0Qyh86KUi8UqIXqUNZmXfD9OongNpODHv2eMM/wzkh5OTogrHEVtwkejP7\nL+APQBLwN+fcPbW0UaKXuFRW5t1UvbZPBtVPDHv2QKtWdZ8UsrK8k0blR+vWXy/rwrI0VFwkejNL\nArYAA4FdwDpgvHPuw2rtnJs9u+rGqakwfXrNnR47BvfeW3N9airceWfE2gc/+YRA165R2//JtK8S\nUxzEUy74yScEunePm3gAgtu3E5gzJ6bxOAcHCo9R+pt7OXzYm9758BE4chgOHE3lT2mX0apVgCNH\nqHh8dfgY399/LyUlod228HZtLVNZ0OXOGieGNq2O8a3376VFqF2LVG+blLRUdk6q2b51i2Ok//ne\nmjeQCcUfDAYJBAIxeX/CbV8lpljE87vf1d7+jjuqrAoGgwT69Qu7fUP3fzLtI5noUxqx7aXAR865\nbaGgFgKjgA9rtCwtrfr8RNMilv9VhOsk2ge3biVw+ulR2//JtA87phjFUy64dWvVk6LP8QAEP/qI\nQIzjMfN67WSW0CGz2oup8N7xIHl51aI6BvzK2//x496hvvoKSoCrb6TKSeHIEfjiAKRvLeGrr+CL\nYigOtT9aCo/vrdm+5BDcdrgEM280UvmJgZawcDHs3Bnk/PMD3kmjBaQlw7c/KCE5ybu7WHJy6E8x\nFdYcp+IEU97+FINeG0pIqtw+GZJaQuHamu1THbQ5UK19tT/1GiefaP//+fLLmuvKymqsqkj0YbZv\n6P5Pun2ENKZHfx0wxDn3w9DzScClzrlbq7WLu9JNXl4eeXl5fodRRTzGBPEZl2L6mnNe7itP/ocP\nf738l7/kMW5cHiUllU4yJ7l8sttA1ZPBsWN5ZGbmVZwI6npUPllEqu2J2q1cmcegQXkkJXntzDjh\ncrRfT0qCQCA+evQi4jMzb+RQy5Y1p5NeuRJGjPAnrnJVPsmUwN13w7Rp3voTPcrK6m8TqXblJ6WD\nB70TZ1mZ96hvOdqvR1JjevSXA3nOuf8KPZ8OuOoXZM0svrrzIiJNRDxcjE0GNuNdjP0MWAtMcM5t\nikRgIiISGSddunHOHTeznwDL+Hp4pZK8iEicifoXpkRExF+Nvv2zmf3NzArN7J1K67LMbJmZbTaz\npWaWWem1GWb2kZltMrPBjT1+LfF0NrMVZva+mb1rZrf6HVPoGC3N7A0z2xCKa1acxJVkZuvNbFE8\nxBM6zn/M7O3Qe7U2HuIys0wzeyZ0jPfN7DKf/593C70/60M/D5rZrXHwPk0zs/fM7B0zm29mqX7H\nFDrO1NDfnW85IVK50sx6h97fLWb2h7AO7pxr1AO4ErgYeKfSunuAO0LLdwK/DS2fD2zAKxmdAWwl\n9KkiUg+gI3BxaLk13nWEc/2MqVJsaaGfycAavO8i+BoXMA2YByzy+3dXKaaPgaxq6/x+nx4Hvhta\nTgEy/Y6pUmxJeF9a7OLz316n0O8uNfT8KWCK3+8TcAHwDtAy9Le3DDgr1nERoVwJvAFcElpegjfM\n/cTHjtAbmVst+A+BnNByR+DD0PJ04M5K7QqAy6L1BxA6xvPAoDiLKQ14E7jEz7iAzsByIMDXid73\n9wn4BGhXbZ2f71MG8O9a1vv+XoX2Pxh4ze+Y8BL9NiArlKAWxcPfHnA98JdKz+8CfgpsinVcNDJX\nhtp8UGn9eODP9R230aWbOmQ75woBnHO7gezQ+tOB7ZXa7QytiwozOwPvDLoG7830NaZQmWQDsBtY\n7pxb53Nc/4v3H77yhRrf36dQPMvNbJ2Z/SAO4uoKfG5mj4VKJXPMLM3nmCobBywILfsWk3NuF3A/\n8Glo/wedc6/4GVPIe8CAUJkkDRiG9+nH77ig4bnydGBHpfU7woktWom+uphf8TWz1sCzwFTn3OFa\nYoh5TM65MudcL7ye9KVmdoFfcZnZt4BC59xG4ERjdf24Wt/fOdcb7w/yx2Y2oJY4YhlXCtAbeDAU\n1xG8Hpfv/6fMrAUwEnimjhhiFpOZtcGbBiUXr3d/qpnd4GdMAM6bf+sevE+vS/BKIsdraxrLuOoQ\nlRiilegLzSwHwMw6AntC63finUnLdQ6tiygzS8FL8nOdcy/EQ0yVOeeKgSDwXz7G1R8YaWYfA08C\n15jZXGC33++Tc+6z0M+9eKW3S/H397cD2O6cezP0/O94iT8e/k8NBd5yzn0eeu5nTIOAj51z+51z\nx4HngCt8jgkA59xjzrm+zrkAcADv2p3vcZ1EDCcVW6QSvVG1V7gIuCm0PAV4odL68aEr8V2Bs/G+\naBVpj+LVsf4YLzGZWfvyK+pm1gq4Fq9G6EtczrmfOee+4Zw7E6/Ot8I5Nxl40Y94yplZWujTGGZ2\nKl79+V18/P2FPlpvN7NuoVUDgff9jKmSCXgn6nJ+xvQpcLmZnWJmhvc+feBzTACYWYfQz28A38Yr\ndfkRV6NyZai8c9DMLg29xzdW2qZuEbi4sADviv8xvF/0d/EuxryCd9ZcBrSp1H4G3hXkTcDgKFx4\n6Y/3sWwj3ke09Xg957Z+xRQ6Rs9QLBvxRgD8v9B6X+MKHecqvr4Y6/f71LXS7+5dYHqcxHUR3lTc\nG4H/wxt143dMacBeIL3SOr9jmhXa/ztAPtDC75hCx1mFV6vfAAT8eK+IUK4E+oT+Nj4C/hjOsfWF\nKRGRBBeri7EiIuITJXoRkQSnRC8ikuCU6EVEEpwSvYhIglOiFxFJcEr0IiIJToleRCTB/X8jG/hq\nyZ00ZQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7efbbdd50358>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(plt_x, plt_y1, label='Training RMSE')\n",
    "plt.plot(plt_x, plt_y2, linestyle='--', color='r', label='Validation RMSE')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def success_rate(pred, true):\n",
    "    cnt = 0\n",
    "    for i in range(pred.shape[0]):\n",
    "        t = np.where(true[i] == 1) # true set\n",
    "        ary = np.intersect1d(pred[i], t)\n",
    "        if ary.size > 0:\n",
    "            cnt += 1\n",
    "    return cnt * 100 / pred.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success Rate at 1: 1.596275\n",
      "Success Rate at 2: 3.259062\n",
      "Success Rate at 3: 4.772198\n",
      "Success Rate at 4: 6.235451\n",
      "Success Rate at 5: 7.515796\n",
      "Success Rate at 6: 8.513469\n",
      "Success Rate at 7: 9.561024\n",
      "Success Rate at 8: 10.325906\n",
      "Success Rate at 9: 11.057532\n",
      "Success Rate at 10: 11.390090\n"
     ]
    }
   ],
   "source": [
    "pred = sess.run(decoded, feed_dict={x:data_x_, train_inputs:list_users})\n",
    "pred = pred * (train_matrix == 0) # remove watched items from predictions\n",
    "pred = np.argsort(pred)\n",
    "\n",
    "for n in range(1, 11):\n",
    "    sr = success_rate(pred[:, -n:], test_matrix)\n",
    "    print(\"Success Rate at {:d}: {:f}\".format(n, sr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success Rate at 1: 2.311274\n",
      "Success Rate at 2: 4.506152\n",
      "Success Rate at 3: 6.551380\n",
      "Success Rate at 4: 8.496841\n",
      "Success Rate at 5: 10.192883\n",
      "Success Rate at 6: 11.606252\n",
      "Success Rate at 7: 13.086132\n",
      "Success Rate at 8: 14.050549\n",
      "Success Rate at 9: 15.114732\n",
      "Success Rate at 10: 15.763219\n"
     ]
    }
   ],
   "source": [
    "pred = sess.run(decoded, feed_dict={x:data_x_, train_inputs:list_users})\n",
    "pred = pred * (train_matrix == 0) # remove watched items from predictions\n",
    "pred = np.argsort(pred)\n",
    "\n",
    "for n in range(1, 11):\n",
    "    sr = success_rate(pred[:, -n:], full_matrix)\n",
    "    print(\"Success Rate at {:d}: {:f}\".format(n, sr))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAP@ 1 : 0.0400731626206\n",
      "MAP@ 2 : 0.0310525440639\n",
      "MAP@ 3 : 0.0316021874885\n",
      "MAP@ 4 : 0.030002909877\n",
      "MAP@ 5 : 0.0311351291431\n",
      "MAP@ 6 : 0.0353924176921\n",
      "MAP@ 7 : 0.0409458994772\n",
      "MAP@ 8 : 0.048742220533\n",
      "MAP@ 9 : 0.057609705807\n",
      "MAP@ 10 : 0.0681741676828\n"
     ]
    }
   ],
   "source": [
    "pred = sess.run(decoded, feed_dict={x:data_x_, train_inputs:list_users})\n",
    "pred = np.argsort(pred)\n",
    "actual = np.argsort(train_matrix)\n",
    "\n",
    "for n in range(1, 11):\n",
    "    sr = mapk(actual=list(actual[:,-n:]), predicted=list(pred[:,-n:]), k=n)\n",
    "    print(\"MAP@\", n, \":\", sr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.],\n",
       "       [ 1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  1.],\n",
       "       [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.],\n",
       "       [ 0.,  0.,  0.,  1.,  1.,  1.,  0.,  0.,  0.,  0.]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_matrix[:10,:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pred @ 2155 : [ 751 2357  764  530  479  813  370  383  106  938]\n",
      "5*   @ 2155 : [8, 29, 57, 66, 74, 127, 142, 270, 279, 286, 386, 424, 561, 599, 696, 697, 736, 754, 847, 849, 872, 879, 938, 977, 1018, 1038, 1040, 1327, 1338, 1347, 1388, 1400, 1663]\n",
      "Pred @ 5787 : [706 488 658 938 383 892 200  38  35 281]\n",
      "5*   @ 5787 : [0, 3, 4, 5, 6, 7, 8, 11, 14, 15, 19, 21, 22, 24, 28, 30, 35, 36, 37, 38, 41, 42, 43, 45, 47, 48, 51, 52, 54, 57, 58, 60, 62, 74, 78, 87, 89, 91, 93, 99, 103, 105, 108, 110, 115, 116, 117, 130, 138, 148, 151, 155, 157, 159, 164, 167, 170, 172, 174, 185, 197, 198, 206, 208, 209, 215, 216, 217, 219, 229, 233, 235, 243, 247, 249, 253, 257, 258, 260, 261, 275, 281, 284, 288, 303, 315, 321, 335, 348, 349, 364, 370, 396, 401, 403, 407, 414, 424, 437, 439, 452, 460, 461, 465, 481, 485, 493, 513, 518, 519, 523, 529, 530, 542, 543, 544, 547, 549, 552, 553, 557, 561, 578, 579, 585, 598, 602, 607, 620, 621, 624, 627, 630, 631, 633, 647, 655, 656, 657, 658, 663, 697, 705, 712, 713, 720, 737, 741, 748, 766, 781, 793, 799, 816, 832, 833, 849, 860, 868, 872, 878, 883, 892, 894, 903, 934, 959, 960, 982, 1019, 1024, 1029, 1033, 1041, 1052, 1057, 1060, 1074, 1096, 1155, 1182, 1222, 1250, 1253, 1257, 1279, 1283, 1290, 1300, 1368, 1400, 1424, 1446, 1451, 1485, 1504, 1516, 1523, 1525, 1627, 1634, 1698, 1704, 1756, 1768, 1793, 1808, 1820, 1871, 1921, 1949, 2253, 2258, 2365, 2625, 3076, 3135]\n",
      "Pred @ 485 : [1078 1079 1080 1081 1082 3231  198  764 2357  106]\n",
      "5*   @ 485 : [5, 25, 43, 132, 209, 315, 368, 1422]\n",
      "Pred @ 1264 : [ 327  139 2357  892  383  764  106   89  237  204]\n",
      "5*   @ 1264 : [0, 15, 16, 33, 37, 43, 54, 58, 68, 89, 105, 115, 132, 169, 203, 204, 237, 284, 301, 311, 323, 369, 404, 434, 485, 564, 583, 725, 850, 892, 926, 932, 1018, 1028, 1037, 1266, 1290, 1325, 1461, 1774, 1925, 1986, 2430, 2543, 2544]\n",
      "Pred @ 1333 : [1081 1082 1083 1075 3231  839  200 2357  383  106]\n",
      "5*   @ 1333 : [8, 13, 17, 305, 321, 822, 1018]\n",
      "Pred @ 4732 : [1077 1078 1079 1080 1081 1072 3231  764 2357  106]\n",
      "5*   @ 4732 : [72, 79, 1313, 1327, 1698]\n",
      "Pred @ 4274 : [ 272  198  751  479  706 1422   53 2357  106  116]\n",
      "5*   @ 4274 : [17, 25, 27, 32, 68, 78, 105, 108, 116, 180, 186, 208, 209, 279, 305, 336, 364, 407, 419, 445, 579, 597, 604, 903, 1046, 1214, 1233]\n",
      "Pred @ 1197 : [1077 1078 1079 1080 1081 1082 1073 3231 2357  106]\n",
      "5*   @ 1197 : [209, 286, 395, 414, 416, 470, 1475, 1694, 1943, 2135]\n",
      "Pred @ 2525 : [1078 1079 1080 1081 1082 3231  198 2357  764  106]\n",
      "5*   @ 2525 : [138, 202, 261, 438, 488, 513, 917, 1231]\n",
      "Pred @ 3964 : [ 839  769  706  198  383 2357  519  479  764  106]\n",
      "5*   @ 3964 : [0, 43, 47, 50, 54, 58, 185, 225, 280, 315, 321, 370, 375, 382, 438, 476, 481, 483, 485, 488, 491, 508, 543, 561, 576, 663, 697, 737, 817, 867, 868, 891, 926, 946, 1017, 1059, 1087, 1468, 1635]\n"
     ]
    }
   ],
   "source": [
    "asd = sess.run(decoded, feed_dict={x:data_x_, train_inputs:list_users})\n",
    "asd[:10,:10]\n",
    "res = np.argsort(asd)\n",
    "\n",
    "for i in range(10):\n",
    "    idx = np.random.randint(total_user)\n",
    "    print(\"Pred @\", idx, \":\", res[idx,-10:])\n",
    "    print(\"5*   @\", idx, \":\", m[idx])\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
