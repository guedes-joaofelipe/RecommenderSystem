{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from caserec.recommenders.rating_prediction.nnmf import NNMF\n",
    "from caserec.recommenders.rating_prediction.ae import AE\n",
    "from keras import initializers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make the import\n",
    "import os, sys\n",
    "import pandas as pd\n",
    "import joblib\n",
    "import numpy as np\n",
    "from caserec.utils.split_database import SplitDatabase\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline \n",
    "from IPython.display import clear_output\n",
    "\n",
    "lib_path = './../Sources/Utilities'\n",
    "if (lib_path not in sys.path):\n",
    "    sys.path.append(lib_path) #src directory\n",
    "\n",
    "from messaging.print_functions import ProgressBar\n",
    "from messaging.telegrambot import Bot\n",
    "bot = Bot(user_credentials='./JFGS.json')\n",
    "\n",
    "# Checking if bot is ok\n",
    "bot.send_message(text=\"Hello, John\")\n",
    "progbar = ProgressBar(bar_length=20, bar_fill='#', elapsed_time=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset, dataset_type = 'MovieLens', '100k'\n",
    "# dataset, dataset_type = 'BookCrossing', 'Standard'\n",
    "# dataset, dataset_type = 'Amazon', 'MoviesTV'\n",
    "# dataset, dataset_type = 'Amazon', 'InstantVideo'\n",
    "# dataset, dataset_type = 'Jester', 'jester'\n",
    "\n",
    "dataset_folder = \"../Datasets/\" + dataset + \"/\" + dataset_type + \"/\"\n",
    "dataset_output_folder = dataset_folder + 'outputs/'\n",
    "\n",
    "df_overall_sparsity = pd.read_csv(dataset_output_folder + 'df_overall_sparsity.tsv', sep='\\t', header=0)\n",
    "df_overall_sparsity.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sparsity_datasets_folder = dataset_output_folder + 'sparsity_datasets/'\n",
    "sparsity_folders = os.listdir(sparsity_datasets_folder)\n",
    "if 'desktop.ini' in sparsity_folders:    \n",
    "    sparsity_folders.remove('desktop.ini')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Evaluation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def eval_model(**kwargs):\n",
    "    from random import choice\n",
    "    from string import ascii_uppercase\n",
    "\n",
    "    predictions_output_filepath = './predictions_output_' + ''.join(choice(ascii_uppercase) for i in range(12)) + '.dat'\n",
    "    \n",
    "    if (kwargs['model_name'].lower() == 'item-knn'):\n",
    "        from caserec.recommenders.item_recommendation.itemknn import ItemKNN\n",
    "\n",
    "        model = ItemKNN(\n",
    "                        train_file=kwargs['train_filepath'], \n",
    "                        test_file=kwargs['test_filepath'], \n",
    "        #                 as_binary=True, # If True, the explicit feedback will be transform to binary\n",
    "                        k_neighbors=kwargs['k_neighbors'],\n",
    "                        similarity_metric=kwargs['similarity_metric'],\n",
    "                        rank_length=kwargs['top_n'])\n",
    "\n",
    "        model.compute(\n",
    "            metrics=None, \n",
    "            as_table=True,\n",
    "            n_ranks=[kwargs['top_n']],\n",
    "            verbose=False)\n",
    "        \n",
    "        eval_results = model.evaluation_results\n",
    "\n",
    "    elif (kwargs['model_name'].lower() == 'nnmf'):  \n",
    "        from caserec.recommenders.rating_prediction.nnmf import NNMF\n",
    "        from caserec.utils.process_data import ReadFile\n",
    "        from caserec.evaluation.rating_prediction import RatingPredictionEvaluation\n",
    "        \n",
    "        predictions_output_filepath = './predictions_output.dat'\n",
    "        \n",
    "        model = NNMF(kwargs['train_filepath'], \n",
    "                      kwargs['test_filepath'], \n",
    "                      factors=kwargs['n_factors'],\n",
    "                     output_file = predictions_output_filepath)\n",
    "        \n",
    "        model.compute(verbose=False)\n",
    "        \n",
    "        # Using ReadFile class to read predictions from file\n",
    "        reader = ReadFile(input_file=predictions_output_filepath)\n",
    "        predictions = reader.read()\n",
    "        \n",
    "        # Creating evaluator with item-recommendation parameters\n",
    "        evaluator = RatingPredictionEvaluation(sep = '\\t', \n",
    "                                               n_rank = [kwargs['top_n']], \n",
    "                                               as_rank = True,\n",
    "                                               metrics = list(kwargs['metrics']))\n",
    "\n",
    "        # Getting evaluation\n",
    "        eval_results = evaluator.evaluate(predictions['feedback'], model.test_set)\n",
    "        \n",
    "        eval_results['MAE'] = model.evaluation_results['MAE']\n",
    "        eval_results['RMSE'] = model.evaluation_results['RMSE']\n",
    "        \n",
    "    elif (kwargs['model_name'].lower() == 'ae'):  \n",
    "        from caserec.recommenders.rating_prediction.ae import AE        \n",
    "        from caserec.utils.process_data import ReadFile\n",
    "        from caserec.evaluation.rating_prediction import RatingPredictionEvaluation\n",
    "        \n",
    "        model = AE(kwargs['train_filepath'], \n",
    "                   kwargs['test_filepath'], \n",
    "                   num_neurons=kwargs['num_neurons'],\n",
    "                   kernel_initializer=kwargs['kernel_initializer'],\n",
    "                   output_file = predictions_output_filepath)\n",
    "        \n",
    "        model.compute(verbose=False)\n",
    "        \n",
    "        # Using ReadFile class to read predictions from file\n",
    "        reader = ReadFile(input_file=predictions_output_filepath)\n",
    "        predictions = reader.read()\n",
    "        \n",
    "        # Creating evaluator with item-recommendation parameters\n",
    "        evaluator = RatingPredictionEvaluation(sep = '\\t', \n",
    "                                               n_rank = [kwargs['top_n']], \n",
    "                                               as_rank = True,\n",
    "                                               metrics = list(kwargs['metrics']))\n",
    "\n",
    "        # Getting evaluation\n",
    "        eval_results = evaluator.evaluate(predictions['feedback'], model.test_set)    \n",
    "        \n",
    "        eval_results['MAE'] = model.evaluation_results['MAE']\n",
    "        eval_results['RMSE'] = model.evaluation_results['RMSE']\n",
    "        \n",
    "    elif (kwargs['model_name'].lower() == 'bprmf'):  \n",
    "        from caserec.recommenders.item_recommendation.bprmf import BprMF\n",
    "        \n",
    "        model = BprMF(kwargs['train_filepath'], \n",
    "                      kwargs['test_filepath'], \n",
    "                      batch_size=30, \n",
    "                      rank_length = kwargs['top_n'])\n",
    "        \n",
    "        model.compute(\n",
    "            metrics=None, \n",
    "            as_table=True)\n",
    "        \n",
    "        eval_results = model.evaluation_results\n",
    "\n",
    "    eval_results['uss_limit'] = kwargs['uss_limit']\n",
    "    eval_results['iss_limit'] = kwargs['iss_limit']\n",
    "\n",
    "    os.remove(predictions_output_filepath)\n",
    "    \n",
    "    return eval_results    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating list of argument to evaluation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_n = 10\n",
    "model_name = 'ae'\n",
    "metrics = ('PREC', 'RECALL', 'NDCG', 'MAP')\n",
    "\n",
    "# Item KNN Parameters\n",
    "k_neighbors = 30\n",
    "similarity_metric= 'cosine'\n",
    "# NNMF Parameters\n",
    "n_factors = 30\n",
    "# Auto encoder parameters \n",
    "kernel_initializer=initializers.random_uniform()\n",
    "num_neurons = 400\n",
    "\n",
    "arr_eval_args = []\n",
    "\n",
    "for index, row in df_overall_sparsity.iterrows():    \n",
    "    progbar.update_progress(index/float(df_overall_sparsity.shape[0]))\n",
    "        \n",
    "#     if (index > 15):\n",
    "#         break\n",
    "        \n",
    "    fold_num = 0\n",
    "    n_folds = 2\n",
    "    \n",
    "    uss_limit = row['uss_limit']\n",
    "    iss_limit = row['iss_limit']\n",
    "    \n",
    "#     if (int(100*uss_limit)%2 == 1) or (int(100*iss_limit)%2 == 1):\n",
    "#         continue\n",
    "    \n",
    "    target_folder = 'usslimit_{}_isslimit_{}'.format(uss_limit, iss_limit)\n",
    "\n",
    "    if target_folder not in sparsity_folders:\n",
    "        print (\"Error findind \" + target_folder + \" folder\")    \n",
    "        break\n",
    "    else:        \n",
    "        # Visualize file content\n",
    "        ratings_filepath = sparsity_datasets_folder + target_folder + '/u.data'\n",
    "        cross_validation_folder = sparsity_datasets_folder + target_folder + '/'\n",
    "\n",
    "        try:\n",
    "        \n",
    "            SplitDatabase(input_file=ratings_filepath, dir_folds = cross_validation_folder, n_splits=n_folds).k_fold_cross_validation()\n",
    "\n",
    "        except : \n",
    "            print (\"Erro em \", ratings_filepath)\n",
    "            break\n",
    "            \n",
    "        fold_path = cross_validation_folder + 'folds/' + str(fold_num)\n",
    "\n",
    "        train_filepath = fold_path + '/train.dat'\n",
    "        test_filepath = fold_path + '/test.dat'\n",
    "\n",
    "        temp_eval_args = {'uss_limit': uss_limit,\n",
    "                          'iss_limit': iss_limit,\n",
    "                          'model_name': model_name,                           \n",
    "                          'metrics': metrics,\n",
    "                          'train_filepath': train_filepath, \n",
    "                          'test_filepath': test_filepath, \n",
    "                          'top_n': top_n, \n",
    "#                           'k_neighbors': k_neighbors, # KNN\n",
    "#                           'similarity_metric': similarity_metric, # KNN\n",
    "#                           'n_factors': n_factors, # NNMF\n",
    "                          'kernel_initializer': kernel_initializer, # AE\n",
    "                          'num_neurons': num_neurons # AE\n",
    "                         }\n",
    "\n",
    "        arr_eval_args.append(temp_eval_args)\n",
    "\n",
    "text = \"Finished creating arr_eval_args for {} on {}/{} dataset in {}\".format(model_name, dataset, dataset_type, progbar.get_elapsed_time())\n",
    "joblib.dump(arr_eval_args, dataset_output_folder + 'arr_eval_args_' + model_name + '.joblib')\n",
    "bot.send_message(text=text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunkIt(seq, num):\n",
    "    avg = len(seq) / float(num)\n",
    "    out = []\n",
    "    last = 0.0\n",
    "\n",
    "    while last < len(seq):\n",
    "        out.append(seq[int(last):int(last + avg)])\n",
    "        last += avg\n",
    "\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(arr_eval_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_chunks = 10\n",
    "arr_chunks_eval_args = chunkIt(arr_eval_args, n_chunks)\n",
    "joblib.dump(arr_chunks_eval_args, dataset_output_folder + 'arr_' + str(n_chunks) + '_chunks_eval_args_' + model_name + '.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'ae'\n",
    "n_chunks = 10\n",
    "chunk_number = 3\n",
    "arr_chunks_eval_args = joblib.load(dataset_output_folder + 'arr_' + str(n_chunks) + '_chunks_eval_args_' + model_name + '.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(arr_chunks_eval_args[chunk_number])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr_chunks_eval_args[chunk_number] = chunkIt(arr_chunks_eval_args[chunk_number], 4)[3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing evaluation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "eval_model(**arr_chunks_eval_args[chunk_number][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bot.send_message(text='Started creating df_eval_metadata for chunk number ' + str(chunk_number))\n",
    "max_evals = len(arr_chunks_eval_args[chunk_number])\n",
    "for index, row in enumerate(arr_chunks_eval_args[chunk_number]):\n",
    "    current_save = int(10*index/float(max_evals)) \n",
    "    \n",
    "    clear_output()\n",
    "    progbar.update_progress(index/float(max_evals))    \n",
    "\n",
    "    print (row)\n",
    "    \n",
    "#     if (index >= 15):\n",
    "#         break\n",
    "    \n",
    "    eval_result = eval_model(**arr_chunks_eval_args[chunk_number][index])\n",
    "\n",
    "    if (index == 0):\n",
    "        colnames = list(eval_result.keys())\n",
    "        df_eval_metadata = pd.DataFrame(columns = colnames)\n",
    "        \n",
    "    df_eval_metadata.loc[index] = [eval_result[x] for x in colnames]\n",
    "\n",
    "    df_eval_metadata.to_csv(dataset_output_folder + 'df_eval_metadata_' + model_name + '_chunk_' + str(chunk_number+11) + '.tsv', sep = '\\t', header = 1, index = 0 )\n",
    "text = \"Finished creating df_eval_metadata for {} on {}/{} dataset (chunk #{}.1)\".format(model_name, dataset, dataset_type, chunk_number)\n",
    "bot.send_message(text=text)\n",
    "# import os\n",
    "# os.system('shutdown -s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Shutting down the computer\"\n",
    "bot.send_message(text=text)\n",
    "import os\n",
    "try:\n",
    "    os.system('shutdown -s')\n",
    "except: \n",
    "    text = \"Couldn't shut down the computer\"\n",
    "    bot.send_message(text=text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_eval_metadata.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_eval_metadata.to_csv(dataset_output_folder + 'df_eval_metadata_' + model_name + '.tsv', sep = '\\t', header = 1, index = 0 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking memory usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "# These are the usual ipython objects, including this one you are creating\n",
    "ipython_vars = ['In', 'Out', 'exit', 'quit', 'get_ipython', 'ipython_vars']\n",
    "\n",
    "# Get a sorted list of the objects and their sizes\n",
    "sorted([(x, sys.getsizeof(globals().get(x))) for x in dir() if not x.startswith('_') and x not in sys.modules and x not in ipython_vars], key=lambda x: x[1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_n = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ranking Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr_rank_metrics = ['PREC@' + str(top_n), 'RECALL@' + str(top_n), 'NDCG@' + str(top_n), 'MAP@' + str(top_n)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uss_limits = df_eval_metadata['uss_limit'].unique()\n",
    "iss_limits = df_eval_metadata['iss_limit'].unique()\n",
    "\n",
    "# column = 'MAP@10'\n",
    "arr_rank_metrics = ['PREC@' + str(top_n), 'RECALL@' + str(top_n), 'NDCG@' + str(top_n), 'MAP@' + str(top_n)]\n",
    "\n",
    "for column in arr_rank_metrics:\n",
    "\n",
    "    arr_prec = np.zeros([len(uss_limits), len(iss_limits)])\n",
    "    for uss_index, uss_limit in enumerate(uss_limits):\n",
    "        for iss_index, iss_limit in enumerate(iss_limits):\n",
    "            arr_prec[uss_index, iss_index] = df_eval_metadata[(df_overall_sparsity['uss_limit'] == uss_limit) & (df_overall_sparsity['iss_limit'] == iss_limit)][column].reset_index(drop = True)[0]\n",
    "\n",
    "    joblib.dump(arr_prec, dataset_output_folder + 'arr_' + column.lower() + '_' + model_name + '.joblib')        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for column in arr_rank_metrics:\n",
    "\n",
    "    arr_prec = joblib.load(dataset_output_folder + 'arr_' + column.lower() + '_' + model_name + '.joblib')        \n",
    "\n",
    "    cmapping = \"jet\"\n",
    "    fig, ax = plt.subplots(figsize=(10,10))\n",
    "\n",
    "    # plt.subplot(1, 1, 1)\n",
    "    cax = plt.imshow(arr_prec, cmap=cmapping)\n",
    "    plt.gca().invert_yaxis()\n",
    "    cbar = plt.colorbar(cax, ticks = [x/100.0 for x in np.arange(0,1000,10)], shrink = 0.83)\n",
    "    # plt.colorbar.set_label('OS', labelpad=-50,  y=1.05, rotation=0, fontsize = label_fontsize)\n",
    "\n",
    "    tick_step = int(10)\n",
    "\n",
    "    ax.set_xticklabels(uss_limits[0:len(uss_limits):tick_step])\n",
    "    ax.set_yticklabels(iss_limits[0:len(uss_limits):tick_step])\n",
    "\n",
    "    plt.xticks(np.arange(0, len(uss_limits), tick_step))\n",
    "    plt.yticks(np.arange(0, len(iss_limits), tick_step))\n",
    "\n",
    "    plt.clim(0, 1)\n",
    "\n",
    "\n",
    "    tick_fontsize = 20\n",
    "    label_fontsize = 25\n",
    "\n",
    "    # # Setting Labels\n",
    "    ax.set_xlabel('Last User Specific Sparsity', fontsize = label_fontsize)\n",
    "    ax.set_ylabel('Last Item Specific Sparsity', fontsize = label_fontsize)\n",
    "    # cbar = fig.colorbar(cax, ticks = [x/100.0 for x in np.arange(0,1000,5)])\n",
    "\n",
    "    plt.xticks(rotation = 'vertical')\n",
    "    ax.tick_params(axis='both', which='major', labelsize=tick_fontsize)\n",
    "    cbar.set_label(column.title(), labelpad=-50,  y=1.08, rotation=0, fontsize = label_fontsize)\n",
    "    cbar.ax.tick_params(labelsize = tick_fontsize)\n",
    "\n",
    "\n",
    "    # Saving figure\n",
    "    filename = '2d-' + column + '-' + model_name + '.png';\n",
    "    fullpath = dataset_output_folder+'Figures/';\n",
    "    print (\"[*] Saving \" + filename + \" figure to \" + fullpath + \" folder...\")\n",
    "    fig.savefig(fullpath + filename, bbox_inches = 'tight')\n",
    "    print (\"[+] Results saved.\")\n",
    "    \n",
    "#     bot.send_photo(bot_credentials['chat_id'], photo=open(fullpath + filename, 'rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rating Prediction Metrics "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr_rating_metrics = ['RMSE', 'MAE']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uss_limits = df_eval_metadata['uss_limit'].unique()\n",
    "iss_limits = df_eval_metadata['iss_limit'].unique()\n",
    "\n",
    "# column = 'MAP@10'\n",
    "arr_rating_metrics = ['RMSE', 'MAE']\n",
    "\n",
    "for column in arr_rating_metrics:\n",
    "\n",
    "    arr_prec = np.zeros([len(uss_limits), len(iss_limits)])\n",
    "    for uss_index, uss_limit in enumerate(uss_limits):\n",
    "        for iss_index, iss_limit in enumerate(iss_limits):\n",
    "            arr_prec[uss_index, iss_index] = df_eval_metadata[(df_overall_sparsity['uss_limit'] == uss_limit) & (df_overall_sparsity['iss_limit'] == iss_limit)][column].reset_index(drop = True)[0]\n",
    "\n",
    "    joblib.dump(arr_prec, dataset_output_folder + 'arr_' + column.lower() + '_' + model_name + '.joblib')        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr_prec.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for column in arr_rating_metrics:\n",
    "\n",
    "    arr_prec = joblib.load(dataset_output_folder + 'arr_' + column.lower() + '_' + model_name + '.joblib')        \n",
    "\n",
    "    cmapping = \"ocean\"\n",
    "    fig, ax = plt.subplots(figsize=(10,10))\n",
    "\n",
    "    # plt.subplot(1, 1, 1)\n",
    "    cax = plt.imshow(arr_prec, cmap=cmapping)\n",
    "    plt.gca().invert_yaxis()\n",
    "    cbar = plt.colorbar(cax, ticks = [x/100.0 for x in np.arange(0,1000,10)], shrink = 0.83)\n",
    "    # plt.colorbar.set_label('OS', labelpad=-50,  y=1.05, rotation=0, fontsize = label_fontsize)\n",
    "\n",
    "    tick_step = int(10)\n",
    "\n",
    "    ax.set_xticklabels(uss_limits[0:len(uss_limits):tick_step])\n",
    "    ax.set_yticklabels(iss_limits[0:len(uss_limits):tick_step])\n",
    "\n",
    "    plt.xticks(np.arange(0, len(uss_limits), tick_step))\n",
    "    plt.yticks(np.arange(0, len(iss_limits), tick_step))\n",
    "\n",
    "    plt.clim(0.2, 2)\n",
    "\n",
    "\n",
    "    tick_fontsize = 20\n",
    "    label_fontsize = 25\n",
    "\n",
    "    # # Setting Labels\n",
    "    ax.set_xlabel('Last User Specific Sparsity', fontsize = label_fontsize)\n",
    "    ax.set_ylabel('Last Item Specific Sparsity', fontsize = label_fontsize)\n",
    "    # cbar = fig.colorbar(cax, ticks = [x/100.0 for x in np.arange(0,1000,5)])\n",
    "\n",
    "    plt.xticks(rotation = 'vertical')\n",
    "    ax.tick_params(axis='both', which='major', labelsize=tick_fontsize)\n",
    "    cbar.set_label(column.title(), labelpad=-50,  y=1.08, rotation=0, fontsize = label_fontsize)\n",
    "    cbar.ax.tick_params(labelsize = tick_fontsize)\n",
    "\n",
    "\n",
    "    # Saving figure\n",
    "    filename = '2d-' + column + '-' + model_name + '.png';\n",
    "    fullpath = dataset_output_folder+'Figures/';\n",
    "    print (\"[*] Saving \" + filename + \" figure to \" + fullpath + \" folder...\")\n",
    "    fig.savefig(fullpath + filename, bbox_inches = 'tight')\n",
    "    print (\"[+] Results saved.\")\n",
    "    \n",
    "#     bot.send_photo(bot_credentials['chat_id'], photo=open(fullpath + filename, 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_model(**temp_eval_args)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ratings.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sparsity_folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filepath = \"{}ratings.csv\".format(folder)\n",
    "# output_folder = '../Datasets/' + dataset + '/' + dataset_type + '/outputs/'\n",
    "# dataset_output_folder = output_folder + 'sparsity_dataset/'\n",
    "\n",
    "# # Visualize file content\n",
    "# # df_whole = pd.read_csv(filepath, sep='\\t', header=0, names=['user_id', 'item_id', 'rating', 'timestamp']) \n",
    "# df_whole = pd.read_csv(filepath, sep='\\t', header=0) \n",
    "# df_whole.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if not os.path.exists(dataset_output_folder):\n",
    "    os.makedirs(dataset_output_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Autoencoder Analysis "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.listdir(dataset_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "analysis_folder = sparsity_datasets_folder + sparsity_folders[-1] + '/folds/0/'\n",
    "\n",
    "tr = analysis_folder + 'train.dat'\n",
    "te = analysis_folder + 'train.dat'\n",
    "\n",
    "model = AE(tr, te, kernel_initializer=initializers.random_uniform())\n",
    "model.compute(verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "step_size = 10\n",
    "max_percentage = 0.8\n",
    "array_eval_neurons = []\n",
    "array_neurons = np.arange(10, int(max_percentage*len(model.items)), step_size)\n",
    "\n",
    "for index, num_neurons in enumerate(array_neurons):\n",
    "    clear_output()\n",
    "    progbar.update_progress(index/float(len(array_neurons)))    \n",
    "    \n",
    "    model = AE(tr, te, num_neurons=num_neurons, kernel_initializer=initializers.random_uniform())\n",
    "    model.compute(verbose=True)\n",
    "    \n",
    "    line = [num_neurons, model.evaluation_results['MAE'], model.evaluation_results['RMSE']]\n",
    "    array_eval_neurons.append(line)\n",
    "    \n",
    "bot.send_message('Finished analsys RMSE and MAE for AutoEncoder')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'ae'\n",
    "df_eval_neurons = pd.DataFrame(data=array_eval_neurons, columns=['neurons', 'MAE', 'RMSE'])\n",
    "df_eval_neurons.to_csv(dataset_output_folder + 'df_eval_neurons_' + model_name + '.tsv', sep='\\t', header=True)\n",
    "df_eval_neurons.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(ncols=1, nrows=1, figsize=(10,10))\n",
    "ax.plot(df_eval_neurons['neurons'], df_eval_neurons['MAE'], label='MAE')\n",
    "ax.plot(df_eval_neurons['neurons'], df_eval_neurons['RMSE'], label='RMSE')\n",
    "ax.legend()\n",
    "ax.grid()\n",
    "ax.set_title('Metrics x Number of Neurons')\n",
    "ax.set_xlabel('Number of Neurons')\n",
    "\n",
    "# Saving figure\n",
    "filename = 'mae_rmse_x_num_neurons' + '-' + model_name + '.png';\n",
    "fullpath = dataset_output_folder+'Figures/';\n",
    "print (\"[*] Saving \" + filename + \" figure to \" + fullpath + \" folder...\")\n",
    "fig.savefig(fullpath + filename, bbox_inches = 'tight')\n",
    "print (\"[+] Results saved.\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
