{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys, re\n",
    "import json, joblib\n",
    "import ipywidgets as widgets\n",
    "import papermill as pm\n",
    "import matplotlib.pyplot as plt \n",
    "import pandas as pd\n",
    "import numpy as np \n",
    "from IPython.display import clear_output    \n",
    "lib_path = './../Sources'\n",
    "if (lib_path not in sys.path):\n",
    "    sys.path.append(lib_path) #src directory\n",
    "from lpsrec.utils import create_gif    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_folder = './Output_Notebooks'\n",
    "if not os.path.exists(output_folder):\n",
    "    print ('Creating folder ' + output_folder)\n",
    "    os.makedirs(output_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters to be used with Papermill\n",
    "model_tag = 'ItemKNN'\n",
    "arr_dataset_tag = ['ML1M']\n",
    "dataset_tag = arr_dataset_tag[0]\n",
    "evaluation_metrics = ['PREC', 'RECALL', 'NDCG', 'MRR', 'MAP']\n",
    "rank_length = 30\n",
    "nodes = 10\n",
    "n_folds = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for dataset_tag in arr_dataset_tag:\n",
    "#     # Parallelize here later    \n",
    "#     for partition in [x+1 for x in range(nodes)]:\n",
    "#         try:            \n",
    "#             print (\"Starting partition {}\".format(partition))            \n",
    "#             pm.execute_notebook(\n",
    "#                './MostPopular Case Recommender Eval on Sparsity Datasets.ipynb',\n",
    "#                 os.path.join(output_folder, 'MostPopular Case Recommender Eval on Sparsity Datasets [{}][{}].ipynb'.format(dataset_tag, partition)),\n",
    "#                 parameters = dict(\n",
    "#                     dataset_tag = dataset_tag,\n",
    "#                     model_tag = 'MostPopular',\n",
    "#                     rank_length = 20,\n",
    "#                     random_state = 31415,\n",
    "#                     evaluation_metrics = evaluation_metrics,\n",
    "#                     bot_alive = True,\n",
    "#                     partition = partition,\n",
    "#                     nodes = 5)\n",
    "#             )\n",
    "#         except Exception as e:\n",
    "#             print (\"Error running dataset {} on partition {}: {}\".format(dataset_tag, partition, e))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "style_dict = json.load(open('./style_dict.json', 'r'))\n",
    "plt.rc('font', **style_dict['font'])\n",
    "# plt.rc('axes.titlesize', fontsize=20)\n",
    "plt.rc('xtick', labelsize=style_dict['tick']['fontsize']) \n",
    "plt.rc('ytick', labelsize=style_dict['tick']['fontsize']) \n",
    "plt.rcParams.update({'figure.max_open_warning': 5})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bot_alive = False\n",
    "dataset_output_folder = os.path.join('.', 'Outputs', dataset_tag)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Grouping all folders eval metadata into a single file.\n",
    "\n",
    "The output should be a df_eval_metadata_fold{} containing all metrics and all sparsity cenarios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for n_fold in np.arange(1, n_folds+1):\n",
    "    for partition in np.arange(1, nodes+1, 1):\n",
    "        analysis_tag = '_'.join([str(x) for x in [rank_length, nodes, partition]])\n",
    "        variables_output_folder = os.path.join(dataset_output_folder, model_tag, 'Variables', analysis_tag)\n",
    "        figures_output_folder = os.path.join(dataset_output_folder, model_tag, 'Figures', analysis_tag)\n",
    "        df_eval_metadata_temp = pd.read_csv(os.path.join(variables_output_folder, 'df_eval_metadata_fold{}_{}.tsv'.format(n_folds, n_fold)), sep = '\\t', header = 0)    \n",
    "        df_eval_metadata = df_eval_metadata_temp.copy() if partition == 1 else df_eval_metadata.append(df_eval_metadata_temp)        \n",
    "    df_eval_metadata.reset_index(drop = True, inplace = True)\n",
    "    analysis_tag = '_'.join([str(x) for x in [rank_length, 'None']])\n",
    "    variables_output_folder = os.path.join(dataset_output_folder, model_tag, 'Variables', analysis_tag)\n",
    "    figures_output_folder = os.path.join(dataset_output_folder, model_tag, 'Figures', analysis_tag)\n",
    "    if not os.path.exists(variables_output_folder):    \n",
    "        os.makedirs(variables_output_folder)    \n",
    "    if not os.path.exists(figures_output_folder):        \n",
    "        os.makedirs(figures_output_folder)\n",
    "    \n",
    "    df_eval_metadata.to_csv(os.path.join(variables_output_folder, 'df_eval_metadata_fold{}.tsv'.format(n_fold)), sep = '\\t', header = True, index = False)    \n",
    "del df_eval_metadata_temp\n",
    "del df_eval_metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have a df_eval_metadata_fold{}, we can create an arr_metrics for each metrics in each folder and append each arr_metric to an arr_metrics_folder.\n",
    "\n",
    "Example: for precision@k\n",
    "\n",
    "    arr_metric_folds = [arr_metric_fold1, arr_metric_fold2, ...] \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_df_eval_metadata = dict()\n",
    "for n_fold in np.arange(1, n_folds+1):\n",
    "    filename = os.path.join(variables_output_folder, 'df_eval_metadata_fold{}.tsv'.format(n_fold))\n",
    "    print (\">> Getting file \", filename)\n",
    "    df_eval_metadata = pd.read_csv(filename, sep = '\\t', header = 0) \n",
    "    dict_df_eval_metadata[n_fold] = df_eval_metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "uss_limits = np.sort(np.array(df_eval_metadata['uss'].unique()))\n",
    "iss_limits = np.sort(np.array(df_eval_metadata['iss'].unique()))\n",
    "rank_lengths = np.arange(1, rank_length+1, 1) # Setting extra rank analysis\n",
    "arr_metrics = np.repeat(None, n_folds)\n",
    "  \n",
    "for rank in rank_lengths: # [1, 2, 3, ..., rank_length]     \n",
    "    dict_arr_metrics = dict()\n",
    "    for m in evaluation_metrics:\n",
    "        dict_arr_metrics[m] = np.repeat(None, n_folds)\n",
    "    arr_rank_metrics = [m + '@' + str(rank) for m in evaluation_metrics]\n",
    "    \n",
    "    for column in arr_rank_metrics: # [PREC@1, RECALL@1, NDCG@1, ...]\n",
    "        print (\"\\n> Processing \", column)\n",
    "        arr_metric = np.zeros([len(uss_limits), len(iss_limits)])\n",
    "        for n_fold in np.arange(1, n_folds+1):                        \n",
    "            for uss_index, uss_limit in enumerate(uss_limits):\n",
    "                for iss_index, iss_limit in enumerate(iss_limits):            \n",
    "                    arr_metric[uss_index, iss_index] = dict_df_eval_metadata[n_fold][(dict_df_eval_metadata[n_fold]['uss'] == uss_limit) & (dict_df_eval_metadata[n_fold]['iss'] == iss_limit)][column].reset_index(drop = True)[0]\n",
    "            # Colocar em cada metrica\n",
    "            dict_arr_metrics[re.split(\"@\", column)[0]][n_fold-1] = arr_metric\n",
    "        filename = os.path.join(variables_output_folder, 'dict_arr_metrics_' + column.lower() + '_' + model_tag + '.joblib')\n",
    "        print (\"> Writing file \", filename)\n",
    "        joblib.dump(dict_arr_metrics, filename)        \n",
    "    \n",
    "    # Elapsed time analysis\n",
    "#     arr_metric = np.zeros([len(uss_limits), len(iss_limits)])\n",
    "#     for uss_index, uss_limit in enumerate(uss_limits):\n",
    "#         for iss_index, iss_limit in enumerate(iss_limits):            \n",
    "#             arr_metric[uss_index, iss_index] = df_eval_metadata[(df_eval_metadata['uss'] == uss_limit) & (df_eval_metadata['iss'] == iss_limit)]['elapsed_time'].reset_index(drop = True)[0]\n",
    "#     joblib.dump(arr_metric, os.path.join(variables_output_folder, 'arr_elapsed_time_' + model_tag + '.joblib'))        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotting mean of all metrics "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cmapping = \"jet\"\n",
    "tick_step = 5\n",
    "figs = {'mean': {}, 'std': {}}\n",
    "for metric in evaluation_metrics:\n",
    "    figs['mean'][metric+'@'] = list()\n",
    "\n",
    "for rank in rank_lengths:#[-1:-2:-1]:\n",
    "    arr_rank_metrics = [m + '@' + str(rank) for m in evaluation_metrics]\n",
    "    for column in arr_rank_metrics:\n",
    "        dict_arr_metrics = joblib.load(os.path.join(variables_output_folder, 'dict_arr_metrics_' + column.lower() + '_' + model_tag + '.joblib'))        \n",
    "        arr_metric = np.mean(dict_arr_metrics[re.split(\"@\", column)[0]], axis=0)\n",
    "\n",
    "        fig, ax = plt.subplots(figsize=(style_dict['figure']['width'],style_dict['figure']['height']))    \n",
    "        cax = plt.imshow(arr_metric, cmap=cmapping)\n",
    "        plt.gca().invert_yaxis()\n",
    "        cbar = plt.colorbar(cax, ticks = [x/100.0 for x in np.arange(0,1000,10)], shrink = 0.83)\n",
    "\n",
    "        ax.set_xticklabels(uss_limits[0:len(uss_limits):tick_step])\n",
    "        ax.set_yticklabels(iss_limits[0:len(uss_limits):tick_step])\n",
    "        ax.set_xticks(np.arange(0, len(uss_limits), tick_step))    \n",
    "        ax.set_yticks(np.arange(0, len(iss_limits), tick_step))    \n",
    "        ax.set_xlabel('Last User Specific Sparsity', fontsize = style_dict['label']['fontsize'])\n",
    "        ax.set_ylabel('Last Item Specific Sparsity', fontsize = style_dict['label']['fontsize'])    \n",
    "        ax.tick_params(axis='both', which='major', labelsize=style_dict['tick']['fontsize'])\n",
    "        cbar.set_label(column.title(), labelpad=-50,  y=1.08, rotation=0, fontsize = style_dict['label']['fontsize'])\n",
    "        cbar.ax.tick_params(labelsize = style_dict['tick']['fontsize'])\n",
    "        plt.clim(0, 1)\n",
    "        plt.xticks(rotation = 'vertical')\n",
    "\n",
    "        filename = '2d-' + column + '_mean.png'       \n",
    "#         fig.savefig(os.path.join(figures_output_folder, filename), bbox_inches = 'tight')\n",
    "        figs['mean'][re.split(\"\\d\", column)[0]].append(fig) \n",
    "        if rank == rank_length: # Send only the target-analysis\n",
    "            bot.send_message(filePath=os.path.join(figures_output_folder, filename)) if bot_alive else ''\n",
    "        plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotting std of all metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cmapping = \"jet\"\n",
    "tick_step = 5\n",
    "for metric in evaluation_metrics:\n",
    "    figs['std'][metric+'@'] = list()\n",
    "    \n",
    "for rank in rank_lengths:\n",
    "    arr_rank_metrics = [m + '@' + str(rank) for m in evaluation_metrics]\n",
    "    for column in arr_rank_metrics:\n",
    "        dict_arr_metrics = joblib.load(os.path.join(variables_output_folder, 'dict_arr_metrics_' + column.lower() + '_' + model_tag + '.joblib'))        \n",
    "        arr_metric = np.std(dict_arr_metrics[re.split(\"@\", column)[0]], axis=0)\n",
    "\n",
    "        fig, ax = plt.subplots(figsize=(style_dict['figure']['width'],style_dict['figure']['height']))    \n",
    "        cax = plt.imshow(arr_metric, cmap=cmapping)\n",
    "        plt.gca().invert_yaxis()\n",
    "        cbar = plt.colorbar(cax, shrink = 0.83)\n",
    "\n",
    "        ax.set_xticklabels(uss_limits[0:len(uss_limits):tick_step])\n",
    "        ax.set_yticklabels(iss_limits[0:len(uss_limits):tick_step])\n",
    "        ax.set_xticks(np.arange(0, len(uss_limits), tick_step))    \n",
    "        ax.set_yticks(np.arange(0, len(iss_limits), tick_step))    \n",
    "        ax.set_xlabel('Last User Specific Sparsity', fontsize = style_dict['label']['fontsize'])\n",
    "        ax.set_ylabel('Last Item Specific Sparsity', fontsize = style_dict['label']['fontsize'])    \n",
    "        ax.tick_params(axis='both', which='major', labelsize=style_dict['tick']['fontsize'])\n",
    "        cbar.set_label(column.title(), labelpad=-50,  y=1.08, rotation=0, fontsize = style_dict['label']['fontsize'])\n",
    "        cbar.ax.tick_params(labelsize = style_dict['tick']['fontsize'])\n",
    "        plt.clim(0, np.max(np.std(dict_arr_metrics[re.split(\"@\", column)[0]], axis=0)))\n",
    "        plt.xticks(rotation = 'vertical')\n",
    "\n",
    "        filename = '2d-' + column + '_std.png'       \n",
    "        fig.savefig(os.path.join(figures_output_folder, filename), bbox_inches = 'tight')\n",
    "        figs['std'][re.split(\"\\d\", column)[0]].append(fig) \n",
    "        if rank == rank_length: # Send only the target-analysis\n",
    "            bot.send_message(filePath=os.path.join(figures_output_folder, filename)) if bot_alive else ''\n",
    "        plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for metric in evaluation_metrics:    \n",
    "    filepaths = [os.path.join(figures_output_folder, '2d-' + column + '_mean.png' ) for column in [metric + '@' + str(rank) for rank in rank_lengths]]\n",
    "    output_filepath = os.path.join(figures_output_folder, '2d-' + metric + '@k_mean' + '.gif' )\n",
    "    create_gif(filepaths, output_filepath, duration=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for metric in evaluation_metrics:    \n",
    "    filepaths = [os.path.join(figures_output_folder, '2d-' + column + '_std.png' ) for column in [metric + '@' + str(rank) for rank in rank_lengths]]\n",
    "    output_filepath = os.path.join(figures_output_folder, '2d-' + metric + '@k_std' + '.gif' )\n",
    "    create_gif(filepaths, output_filepath, duration=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_metric = lambda metric, k, stat: figs[stat][metric + '@'][int(k)-1]\n",
    "widgets.interact(show_metric, stat=['mean', 'std'], k=widgets.IntSlider(min=1, max=rank_length, step=1, value=10), figs=figs, metric=evaluation_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_eval_metadata.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_time_uss = df_eval_metadata[['uss', 'elapsed_time']].groupby(['uss']).mean().reset_index(drop = False)\n",
    "df_time_iss = df_eval_metadata[['iss', 'elapsed_time']].groupby(['iss']).mean().reset_index(drop = False)\n",
    "df_time_uss.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(style_dict['figure']['width'],style_dict['figure']['height']))    \n",
    "ax.plot(df_time_uss['uss'], df_time_uss['elapsed_time'], label='USS')\n",
    "ax.plot(df_time_iss['iss'], df_time_uss['elapsed_time'], label='ISS')\n",
    "ax.set_ylabel('Elapsed Time (s)', fontsize = style_dict['label']['fontsize'])\n",
    "ax.grid(True)\n",
    "ax.legend()\n",
    "filename = '2d-elapsed_time_mean_uss_iss.png'       \n",
    "fig.savefig(os.path.join(figures_output_folder, filename), bbox_inches = 'tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr_metric = joblib.load(os.path.join(variables_output_folder, 'arr_elapsed_time_' + model_tag + '.joblib'))        \n",
    "\n",
    "fig, ax = plt.subplots(figsize=(style_dict['figure']['width'],style_dict['figure']['height']))    \n",
    "cax = plt.imshow(arr_metric, cmap=cmapping)\n",
    "plt.gca().invert_yaxis()\n",
    "cbar = plt.colorbar(cax, shrink = 0.83)\n",
    "\n",
    "ax.annotate('Min: {:.02f}\\nMax: {:.02f}'.format(np.min(arr_metric), np.max(arr_metric)), xy = (0, 0), fontsize=style_dict['label']['fontsize']-2, color='white')\n",
    "ax.set_xticklabels(uss_limits[0:len(uss_limits):tick_step])\n",
    "ax.set_yticklabels(iss_limits[0:len(uss_limits):tick_step])\n",
    "ax.set_xticks(np.arange(0, len(uss_limits), tick_step))    \n",
    "ax.set_yticks(np.arange(0, len(iss_limits), tick_step))    \n",
    "ax.set_xlabel('Last User Specific Sparsity', fontsize = style_dict['label']['fontsize'])\n",
    "ax.set_ylabel('Last Item Specific Sparsity', fontsize = style_dict['label']['fontsize'])    \n",
    "ax.tick_params(axis='both', which='major', labelsize=style_dict['tick']['fontsize'])\n",
    "cbar.set_label('Elapsed Time (s)', labelpad=-90,  y=1.08, rotation=0, fontsize = style_dict['label']['fontsize'])\n",
    "cbar.ax.tick_params(labelsize = style_dict['tick']['fontsize'])\n",
    "plt.xticks(rotation = 'vertical')\n",
    "plt.clim(np.floor(np.min(arr_metric)), np.ceil(np.max(arr_metric)))\n",
    "filename = '2d-elapsed_time.png'       \n",
    "fig.savefig(os.path.join(figures_output_folder, filename), bbox_inches = 'tight')\n",
    "figs[re.split(\"\\d\", column)[0]].append(fig) \n",
    "if rank == rank_length: # Send only the target-analysis\n",
    "    bot.send_message(filePath=os.path.join(figures_output_folder, filename)) if bot_alive else ''\n",
    "#         clear_output()\n",
    "# plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__________________"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cmapping = \"jet\"\n",
    "# tick_step = 5\n",
    "# figs = dict()\n",
    "# for metric in evaluation_metrics:\n",
    "#     figs[metric+'@'] = list()\n",
    "\n",
    "# for rank in rank_lengths:\n",
    "#     arr_rank_metrics = [m + '@' + str(rank) for m in evaluation_metrics]\n",
    "#     for column in arr_rank_metrics[:1]:\n",
    "#         arr_metric = joblib.load(os.path.join(variables_output_folder, 'arr_' + column.lower() + '_' + model_tag + '.joblib'))        \n",
    "\n",
    "#         fig, ax = plt.subplots(figsize=(style_dict['figure']['width'],style_dict['figure']['height']))    \n",
    "#         cax = plt.imshow(arr_metric, cmap=cmapping)\n",
    "#         plt.gca().invert_yaxis()\n",
    "#         cbar = plt.colorbar(cax, ticks = [x/100.0 for x in np.arange(0,1000,10)], shrink = 0.83)\n",
    "\n",
    "#         ax.set_xticklabels(uss_limits[0:len(uss_limits):tick_step])\n",
    "#         ax.set_yticklabels(iss_limits[0:len(uss_limits):tick_step])\n",
    "#         ax.set_xticks(np.arange(0, len(uss_limits), tick_step))    \n",
    "#         ax.set_yticks(np.arange(0, len(iss_limits), tick_step))    \n",
    "#         ax.set_xlabel('Last User Specific Sparsity', fontsize = style_dict['label']['fontsize'])\n",
    "#         ax.set_ylabel('Last Item Specific Sparsity', fontsize = style_dict['label']['fontsize'])    \n",
    "#         ax.tick_params(axis='both', which='major', labelsize=style_dict['tick']['fontsize'])\n",
    "#         cbar.set_label(column.title(), labelpad=-50,  y=1.08, rotation=0, fontsize = style_dict['label']['fontsize'])\n",
    "#         cbar.ax.tick_params(labelsize = style_dict['tick']['fontsize'])\n",
    "#         plt.clim(0, 1)\n",
    "#         plt.xticks(rotation = 'vertical')\n",
    "\n",
    "#         filename = '2d-' + column + '.png'       \n",
    "#         fig.savefig(os.path.join(figures_output_folder, filename), bbox_inches = 'tight')\n",
    "#         figs[re.split(\"\\d\", column)[0]].append(fig) \n",
    "#         if rank == rank_length: # Send only the target-analysis\n",
    "#             bot.send_message(filePath=os.path.join(figures_output_folder, filename)) if bot_alive else ''\n",
    "# #         clear_output()\n",
    "#         plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (arr_metric.shape)\n",
    "arr_metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uss_limits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "10*int(np.min(arr_metric)*10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[x/100.0 for x in np.arange(10*int(np.min(arr_metric)*10), 101, 10)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(style_dict['figure']['width'],style_dict['figure']['height']))    \n",
    "cs = plt.pcolormesh(uss_limits, \n",
    "                  iss_limits, \n",
    "                  arr_metric, \n",
    "                  cmap=cmapping,\n",
    "#                   linewidths=(0,),\n",
    "#                   colors='black',\n",
    "#                   levels=[x/100.0 for x in np.arange(0, 101, 2)]\n",
    "                   )\n",
    "# ax.clabel(cs, inline=False, fontsize=20, color='k')\n",
    "# plt.clim(0, 1)\n",
    "# cs.cmap.set_over('red')\n",
    "# cs.cmap.set_under('blue')\n",
    "fig.colorbar(cs, ax=ax, ticks=[x/100.0 for x in np.arange(0, 101, 10)])\n",
    "ax.grid(c='k', ls='-', alpha=0.5)\n",
    "cs.changed()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(style_dict['figure']['width'],style_dict['figure']['height']))    \n",
    "cax = plt.imshow(arr_metric, cmap=cmapping)\n",
    "plt.gca().invert_yaxis()\n",
    "cbar = plt.colorbar(cax, ticks = [x/100.0 for x in np.arange(0,1000,10)], shrink = 0.83)\n",
    "\n",
    "ax.set_xticklabels(uss_limits[0:len(uss_limits):tick_step])\n",
    "ax.set_yticklabels(iss_limits[0:len(uss_limits):tick_step])\n",
    "ax.set_xticks(np.arange(0, len(uss_limits), tick_step))    \n",
    "ax.set_yticks(np.arange(0, len(iss_limits), tick_step))    \n",
    "ax.set_xlabel('Last User Specific Sparsity', fontsize = style_dict['label']['fontsize'])\n",
    "ax.set_ylabel('Last Item Specific Sparsity', fontsize = style_dict['label']['fontsize'])    \n",
    "ax.tick_params(axis='both', which='major', labelsize=style_dict['tick']['fontsize'])\n",
    "cbar.set_label(column.title(), labelpad=-50,  y=1.08, rotation=0, fontsize = style_dict['label']['fontsize'])\n",
    "cbar.ax.tick_params(labelsize = style_dict['tick']['fontsize'])\n",
    "plt.clim(0, 1)\n",
    "plt.xticks(rotation = 'vertical')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
